{"pages":[{"title":"About","text":"My name is Carl Scarlett. I’ve been a professional software developer since 1995. About meI grew up learning about software and gaming with the Atari 2600, Commodore 64, Commodore Amiga, and the golden arcade era of the 1980’s and 90’s. I knew making games is what I wanted to do and I spent my youth years analysing and imagining algorithms and interfaces needed for various game mechanics. I developed an intuition for which challenges would persist and be obstacles to overcome once I began developing games of my own. Using numerous lone-wolf game-making heroes as a guide, I practiced for countless hours honing my skills in programming, music, graphics, and design. In those days mastery multiple disciplines was key to becoming a successful game developer. Game development was not an available education option back then. When I left high school I sought education in everything computing offered and chose a university path to keep the broadest computing career options open. This culminated into a Bachelor’s degree with a double major in Computer Science and Information Technology from the University of Western Australia. Times were tough. My family was poor and I live in the most isolated city in the world. Job opportunities in the field were hard to find and the work relatively menial. As my family fell into bankruptcy, divorce and dysfunction, I worked my way up from below minimum wage in the local market. I survived through times where I couldn’t afford to eat every day of the week. I am fortunate to have kept a roof overhead and a computer on hand for study and practice. My career became key to my survival and consumed all my focus and energy. I stayed as employable as possible by learning whatever skills were needed to hold a steady paycheck. Where there was a skill gap I would fill it, finding work mostly designing and developing front end UI that users understood implicitly. This kept me employed for many years as I honed my craft. Though I adore front end work I yearned for more on-the-job experience in back end and distributed programming. However I continued following the skills gap (and the money), and great front end developers were always in short supply. I kept an eye on the internet and the cloud as it formed and coalesced, and nurtured my skills in those areas as best I could. When a break finally came I spent an intense period of quality time building distributed systems and services in AWS. Living the DreamDuring this period I managed to find and marry the most amazing woman. Our partnership bore two beautiful daughters and an amazing family home. I had achieved stability in both career and personal life. After about 20 years of career-time it came as quite a shock to realise I had forgotten what drove me into computing in the first place; making games. I had overcome the tough times but the journey had taken me tangential to my goals. From that day I began investing all spare time into games development. Career-wise I continue to hold full-time employment as an accomplished software engineer and enjoy making the world a better place through software. Priority-wise my partner and I put our children and family first. Together we strive to educate and raise our children to become intelligent, independent, and resilient. Following that, for me, it’s all about my gaming side projects (#zodproj, Twitter). Some days I get half an hour, sometimes an hour an entire fortnight. Progressing my dream is painstakingly slow but I move with unwavering intent. Though the volume of quick-progress stories from full-time, educated developers on social media is difficult to bear, I steel myself against thoughts of impostor syndrome. I have the occasional bad day, but I know from experience it will happen when I keep putting in the work. I still live in the most isolated city in the world but I have deep experience in multiple disciplines and undeniable drive. SummaryI am a designer of software, distributed, UI and UX systems.I am an architect, designer, and engineer.I am an accomplished applications developer.I am an indie dev and game dev.I am disciplined and determined.I am a specialist all-rounder. Me on the WebHere are links to some of my other content on the Web. Github Twitter LinkedIn YouTube AngelList Speaker Deck About this SiteThis website is built using Hexo, a static website generator built with [Node.js][node]. I use the Icarus theme, which I’ve modified to suit my needs. Text content is created using Markdown. I use VS Code for text editing and task running (such as site generation), and a personally licensed version of Affinity Designer for image editing.","link":"/about/index.html"}],"posts":[{"title":"Choosing a Game Engine","text":"I’d decided to use someone else’s engine, but how would I choose which one? Pixel 2 Sample Gallery Image by Allison Johnson BackgroundMy last post talked about how I stopped developing my own game engine and game targeting Windows mobile and tablets, and was looking for someone else’s engine to build a game with targeting Android and iOS mobiles, and probably tablets. I was leaving 15 months of work behind to move into more populated marketplaces. I needed to quickly decide which engines to investigate, how to choose the best fit for my circumstances, and start learning and developing on the chosen engine. My fixed requirement of developing offline during weekday work commutes still stood and would be a major factor in the both the initial shortlist and final choice. This time around I have a solid portable with plenty of space, so engines with heavy footprints could be on the table. The ShortlistUnity and Unreal Engine were the obvious giants of the industry to evaluate, and I also knew of Game Maker. I’ve seen lovable games successfully created with all three of these engines. Googling around I saw and discarded a plethora of unsuitable engines due to lack or wrong target platforms, lack of developer and/or community support, lack of features and buggy functionality, etc. One engine that continuously appeared in my research and resisted being discarded was Godot, which became a fourth candidate. So I ended up with a list of 4 engines to investigate: Unity Unreal Engine Game Maker 2 Godot 3 The Evaluation ProcessI didn’t have the luxury of working through the list one at a time, building something non-trivial to test the capabilities of each engine. Unfortunately I have very limited time available; certainly not enough to spend hours learning multiple engines (as much as I would love to). Up-front research was required so I read a lot of experiences of other people, crawled through multiple forums to gauge community activity, and slowly figured out what about each engine could be applied to my own circumstances. Knowing my eventual choice could lock me in for a number of lengthy projects, I didn’t want to risk wasting time on an engine that wouldn’t stand the test of time. That didn’t narrow the field at all so I tried to define a requirements list. The current game I plan to build is a top down 2D shooter with quite a few sprites flying around, so the engine needed to accommodate building 2D games and perform well. This knocked Unity and Unreal down a notch because Game Maker and Godot have first class support for 2D while the former two seem to hack 2D from 3D contexts. Another requirement of mine was a minimal learning curve, considering I would be mainly developing offline. I had to relax this requirement a little because when learning new tools you generally lean on online community support and code samples. I settled for looking at the solution architectures that resulted from using different engines and whether they made sense to me. I have seen Unreal development in the hands of an expert first hand in the past and I was left feeling the sub-systems didn’t fit together naturally. I was impressed with what I saw in Godot’s SceneTree and node structure, which felt more synergistic and all-encompassing. This seemed more like where my own engine was evolving to and I just felt more comfortable with it. Given I wouldn’t have to shift my thinking much if I went with Godot, it edged out Game Maker and took a slim lead. At this point Godot was shaping up as a real contender, so I began watching Godot tutorials whenever I could to get a feel for the development process while I continued with my evaluation. I carefully considered how much online support I would need as I developed. To say I’m completely offline while developing is not entirely true; I do whip out my phone and use a data plan to browse for solutions to specific problems when I need to. However phone plans in Australia are notoriously expensive (and mine is no different), so I didn’t want to use my mobile as a hot-spot for downloading large assets from in-built asset stores. This took Unity down another notch, and I began to sour on the idea of going with the bigger engines altogether. I did a broad-sweeping analysis of the community collaboration channels supporting the various engines. Unsurprisingly there was huge amount support for Unity and to some extent Unreal, but I found examples of different issue resolutions to the same problem depending on major and minor version releases. This brittleness sent my spidey-sense tingling, because if I had to research something online I would need to find the right solution fast and not be caught out by version issues. Finally I looked at financial costs. There were a variety of concerns thrown by all engines, from initial outlay costs, to costs for purchasable modules through stores, to costs for building on target platforms, to shares of revenue for successful games that crossed income thresholds. Apart from initial cost outlay, there seemed a number of imaginative ways to extract money from developers for services. I don’t have too much of a problem with that, but for an indie developer like me just starting out I felt any money I made would be paid for in blood and sweat, and I didn’t feel like handing over any cash just yet. Godot won this category by a long shot, being free to use, free to deploy, and with a licencing model that pretty much guaranteed the free model would remain throughout the life of the product. Putting all this together there was a clear winner; Godot was the best match for my circumstances with air visible between it and it’s next competitor (Game Maker). Due DiligenceI wasn’t quite ready to jump on in however. Choosing an engine was one thing, but now I just something to scrutinise to see if it really would be the right fit for me. My biggest concern was the offline aspect of my development process. Godot hosts online documentation as a static website, which allowed me to scrape it using a website copier like WebHTTrack and hosting it rendering it offline locally. The documentation source is also available to allow community submissions and language translations; another positive for community engagement. This would help if when needed to search the “online” docs while offline (search in static websites online is still generally implemented as an online service). Further, Godot also employs a built-in help system Godot with context-sensitivity and search features. This earned it another big tick from me. My next concern was feature support. At the time version 3 was relatively new and is a full rewrite. Where Godot 2 was feature rich and complete enough to be fit for purpose, I had to assure myself that version 3 had ported enough features to allow me to get started. Fortunately it seemed so, and had been around long enough that some fantastic tutorial content from people like Kids Can Code and GDQuest had already been made to teach Godot 3. Community efforts like this showed that version 3 was already in a great state to start my development project. My final concern was to find out about the development of the Godot 3 engine itself. Would the small dedicated development team and enthusiastic community contributors be able to port and create stable features at a reasonable pace, or was there a possibility I could “catch up” and need features from the engine before they had completed? My concern was easily dismissed; the team behind Godot had just secured enough Patreon funding to pay their team full time, and between the time I discovered this fact and the time of writing I’ve been really impressed at the speed which features are emerging, how the operation is being run, and how transparent the team is about progress via social media such as blog posts and Twitter. My ChoiceThere was no doubt at this point; Godot 3 was a clear winner and would be the engine I would build my game with. The only lingering doubts I had was whether I could make the leap between using my own engine (which I knew intimately) and using somebody else’s (which I was pretty clueless on how to work at this point). I wouldn’t find out until I tried it out…it was time to get back to development. If you want to know more about what I’ve been working on, check out my #zodproj posts on Twitter or read through my previous blog posts.","link":"/2018/05/02/choosing-a-new-game-engine/"},{"title":"Instrumenting the Accelerometer","text":"Converting my accelerometer capability test from UWP to Godot 3. Shutterstock Image (recoloured) BackgroundHaving successfully ported my touch manipulation capability test from UWP to Godot 3 (the subject of my last post) it was time to prove the capability of another sensor using Godot; the accelerometer. I haven’t posted about this test before (even on my you-tube channel), so let me set the scene for you. The game I’m developing will be controlled by tilting a smartphone or tablet in the real world. Even in their cheapest form, these devices tend to have an accelerometer sensor. Some expensive ones have additional sensors such as gyroscopes and magnetometers which can also be used to determine/enhance how the device is tilted in space. Though prior research and testing I discovered I can get the degree of control I need for the game I’m building using the accelerometer alone. Initially this was revealed with a capability test (the remake of which is the subject of this post), but also later when I developed playable parts of the game on the previous game engine I was developing myself (see my first post). Accelerometer Crash CourseImagine the screen you’re reading this on is a device with an accelerometer. The accelerometer in this device defines it’s movement through real world space using a 3D cartesian coordinate system. It defines an x, y, and z axis through the screen as follows: the x axis passes across the face of the screen from left to right the y axis passes across the face of the screen from top to bottom the z axis passes through the screen from back (behind) to front NOTE If you’re wondering, the origin of the axes (the point where all three axes intersect) is the top-left hand corner of the screen. However that doesn’t matter when we are talking about readings from the accelerometer. The accelerometer measures the acceleration of the device in real world space on these three axes at the time a reading is taken. Readings from the accelerometer are split into x, y, and z values. The accelerometer sensor is constantly measuring so it can provide values any time you take a reading. The Gravity of the SituationThe key thing to understand is that gravity is always affecting these readings. Unless you’re lucky enough to be holding this device in outer-space or on another world, the force of Earth’s gravity is constantly accelerating the device towards the centre of the Earth. If the device is placed on a stationary table and you read the accelerometer you’ll find a force of 9.8 meters per second towards the centre of the Earth being reflected in the results. That’s the amount of force gravity is exerting on the device and the direction in which it is being exerted. This fact can be used to determine the orientation of the device in the real world, and therefore how much the device is tilting. This amount of tilt is what is used to control the game. Visualising the ReadingFor the purpose of this capability test, the screen is a 2D surface on which I want to show the 3D reading from the accelerometer sensor. I chose to represent the x and y component of each reading as a line from the centre of the screen pointing in the same x and y direction, with a length reflecting the strength of the reading in that direction. I also chose to draw a circle around the screen centre to indicate where a force of 9.8 metres per second is, so that if the screen was perfectly stationary and upright (where z == 0) the line would terminate at the circle. This lets me demonstrate rotation around the device’s z axis. For showing the z value of the reading I needed to be creative. I chose to represent the z component by changing the fill colour of the circle. When the z reading is zero, the circle is transparent. When the screen is facing the centre of the Earth, the circle is solid green. When the screen is facing away from the centre of the Earth, the circle is solid red. As the screen tilts towards and away from the centre of the Earth, the colour gradually changes between these three colour values. The end result is that it becomes easy to see which direction the centre of the Earth is on the device as you tilt it through space. It’s quite difficult to explain in words, so let’s jump ahead and see a video of this in action after this capability test has been implemented. Accelerometer capability test in Godot on my Android phone Reading the Accelerometer in GodotIn UWP the accelerometer sensor is exposed in the Windows.Devices.Sensors. You define the interval at which readings will be taken, and an event handler that will receive each readings. In the previous implementation of this capability test I wrapped this sensor object in a service that stored the values as readings came in, and the application simply read from this service when it is was ready to take a reading. In Godot 3, the accelerometer sensor is exposed as a function get_accelerometer() on the Input singleton, which returns a Vector3 containing the x, y, and z values. At the time I was coding, the Godot help system described the functionality of this method as: If the device has an accelerometer, this will return the movement. I knew my Surface Pro (where I do all my coding) had an accelerometer from when I wrote my UWP version of the capability test. When I read the sensor in Godot however, I was only getting zeros. Confused and unable to figure it out myself, I turned to Godot Discord for help. Some friends there did some digging and found the get_accelerometer() method only returns values when you export your code and run it on a device where an implementation has been written. Though it’s fortunate that both UWP and Android (my primary targets) have this implementation, being forced to export the code to test it every time is far from ideal. Unless I came up with a plan, I wouldn’t have a tight development loop and progress on my entire game would be painstakingly slow. MAKING IT BETTER The documentation totally misled me here but there’s a couple of things I can do to make it better. The first is to submit a pull request on the documentation to write better text and prevent confusion. The second is to request for the Godot development team (or community contributor) to implement accelerometer support within the editor (if the device supports it). I plan to take an action on these. Tightening the Development LoopI wanted to simulate moving the device in real space while in the editor, but I knew get_accelerometer() would only return zeroes when running there. However I could make an accelerometer sensor node (a facade) that exposed readings from get_accelerometer() when it was returning values, and create touch controls to manually update the values when get_accelerometer() wasn’t returning values. Then if I only consume the facade my application would work in both scenarios. I designed a touchscreen vertical slider for the left edge of the screen whose value controlled the x and y readings on the sensor facade. Dragging up and down the slider would change its value between 0 and 360, which I would transform into a 2D direction in x and y using sine and cosine trigonometric functions. I allowed the slider to cycle through this range several times down the screen, and put a marker at every zero point. Similarly I designed a touchscreen horizontal slider for the bottom edge of the screen to change it’s value between -180 and 180 and use it to control the z reading on the sensor facade. Using what I had learned about touch input from my manipulations test to quickly knocked out TouchSliderVertical and TouchSliderHorizontal nodes. I created an AccelerometerSensor node (the facade) that would read from get_accelerometer() a few times when it started up, and if it didn’t get a non-zero reading it would determine that the accelerometer sensor wasn’t present. It would set a boolean flag to indicate if the sensor was available, and not read the sensor further if it determined it wasn’t there. This flag was also used to disable the touch sliders if the sensor was available (so the sliders wouldn’t appear if they weren’t required). Putting it all TogetherNow it was time to put all this into a scene. I built a SensorDisplay node to draw the line and the circle, using the readings from the AccelerometerSensor facade (node). A script on the root node connected up all the necessary signals to marshall readings from the touch sliders to the facade, and to disable them if the sensor was present. I even threw in a DebugLabel to show the state of the AccelerometerSensor. Here’s a video of it all hooked up and running from the Godot editor. You can see the touch sliders in action, how they modify the readings of the accelerometer facade, and thus drive the display. Accelerometer capability test in Godot running from the Editor Smoothing Sensor Readings with FiltersI know from experience that raw readings from an accelerometer sensor are noisy and not suitable for using as input in most cases. The way around this is to filter the readings to smooth out the values. There are many strategies that can be employed to smooth the data, and choosing the one that works for you takes some experimentation. From previous efforts I’ve found a simple low-pass filter is adequate for my needs. Such a filter uses both the previous value and the current reading, performs some simple mathematics and creates a new value. This is remarkably easy to do, and given a good dampening coefficient (again found through trial and error) the readings become smoother and more useful. 12345678910var x = 0.0var y = 0.0var z = 0.0var coefficient = 0.35func _process(delta): var reading = Input.get_accelerometer() x = x + coefficient * (reading.x - x) y = y + coefficient * (reading.y - y) z = z + coefficient * (reading.z - z) A simple low-pass filter to smooth readings from the accelerometer GDSCRIPT VS C# It’s worth comparing the implementation of this filter with the implementation from my previous capability test in C# on UWP. In the previous test I separated the accelerometer facade and the smoothing function, then replicated values in a view model and synchronised them with the sensors own ReadingChanged event and the INotifyPropertyChanged event. The implementation was spread across multiple files, and while technically and architecturally acceptable, the simplicity of the implementation in GDScript gives me comfort and confidence in moving ahead on the Godot platform. Exporting to Target PlatformsYou’ve already seen the end result on Android in the video earlier in this post. While exporting to my phone (another device) may feel more complex, it’s actually the easier export path. Once setup, the Android export is as simple as clicking a button in Godot and it’s all done for you. Exporting to UWP is more difficult as Godot doesn’t handle bumping up the version number or completely signing the exported file out of the box. It does handle signing with a developer key automatically on export which is something, however to run the result on Windows you also need to sign the resulting file with a certificate to create a trusted executable. I currently do the non-Godot actions on the command line, and I plan to take some time to automate this with script in the future. Despite the clunky export process, once you have the executable it works just as well as the Android version. Accelerometer capability test in Godot running on my Surface Pro ConclusionAlthough it wasn’t as issue-free a journey as I was expecting the result is pretty satisfying. I was again impressed with the ease at which the Godot solution came together, and hopefully making the accelerometer work when running from the editor can be added to the platform in the future to make the developer experience even better. Coding this capability test in Godot was all too easy and the challenges I encountered were relatively simple to resolve. I was helped greatly with my experience having built this test before, however it’s encouraging to know that if you know what you want to build then building it in Godot is a pleasurable and productive experience.","link":"/2018/09/03/instrumenting-the-accelerometer/"},{"title":"Waiter...my Menu is all GUI!","text":"I built a front end UI for my capability tests. BackgroundAs I ported my accelerometer test (the subject of my last post), I realised creating an app for each test on my Android phone was going to be problematic. When I originally built these tests as UWP apps for Windows Phone in Visual Studio, deployment was easy. It was cheaper in time and effort to just deploy a new app each time rather than building a UI and hosting them in a single app. However deploying to UWP and Android in Godot has different overheads. It takes significantly more effort to configure the project, obtain signing certificates, and do the app signing. Having gone through the pain once I knew I didn’t want to spend time jumping through those hoops every time I created a new test app. It was time to build myself a small UI to choose which test to run within the bounds of a single app, and a simple menu is perfect for this purpose. Designing the MenuBecause the next capability test I will convert requires knowledge of Godot UI control nodes, exploring them now on something simple is a logical first step. A menu that fulfills my current need will show an app title, and a button for each test to choose from. Each button will show the title of the capability test it represents and an image representing what the test looks like when it’s running. I decided to make the menu content dynamic so that updating it with each new test I build will be trivial. A data driven menu is the way to go in this case, so I created an array of dictionary objects with properties of name, thumbnail (image filepath) and path (scene filepath). This is the minimum data each button needs to render and run a test when pressed. 123456789101112var Apps = [ { name = \"Touch Manipulation\", thumbnail = \"res://game/menu/thumbnails/ManipulationTest.png\", path = \"res://touchInput/touchInput.tscn\", }, { name = \"Accelerometer\", thumbnail = \"res://game/menu/thumbnails/AccelerometerTest.png\", path = \"res://sensorInput/sensorInput.tscn\", },] The data to define the app buttons on the menu Scene ControlOn each test I’ve built in Godot up to this point, I’ve configured the project’s main scene (the one that runs when you hit F5) to be the test I was actively working on. Now it was time to implement things properly; constructing a game root node with dynamic loading and unloading of child nodes, thereby changing the scene that is being displayed. When the app starts the game’s root node is pre-loaded with the Menu scene, and when a test is chosen in the menu the game will remove the Menu scene and add the scene of the chosen test. I created a separate game folder so that the implementation of the game logic (loading and unloading children etc) wouldn’t bleed into the implementation of each test. For the time being I kept my tests in their own folders directly under the res:// root folder, though it’s likely I’ll reorganise this to avoid clutter as I add more tests. Here’s how this looks on disk: The Menu SceneAs stated earlier the Menu will show the app title and a button for each test. A left-aligned title with a nice big font will be at the top. In the remaining space underneath I will lay buttons out horizontally across the screen, wrapping to the next line when there are too many to fit. Some space around these pieces will let the UI breathe. To achieve this I used a MarginContainer with some healthy margins to provide the breathing room. To that node I added a VBoxContainer containing a Label node for the app title and a GridContainer for the buttons. I put the GridContainer inside its own MarginContainer so I can specify additional space between the buttons and app title. The SceneButton SceneI designed each button to have a centred image at the top, the test name (title) centred horizontally at the bottom, and a border around them both for aesthetics. I used a Panel to give the illusion of a border, though it’s really just a big coloured box taking up the whole background. Inside that I created a Button node inside a MarginContainer for creating this border illusion. The Button node contains a TextureRect and Label node inside another VBoxContainer to achieve a vertical layout, and each is wrapped in its own MarginContainer to get the spacing just right. I called this scene a SceneButton because it’s a generic button that defines which test scene the game will load when clicked. NOTE In hindsight it may have been better to put the Panel and MarginContainer inside the Button node, which would have made the entire button clickable. For the purpose of this post let’s call this a “feature” and move on. Controlling Scene Loading with SignalsThe SceneButton fire’s a pressed signal that is received by the game node when clicked, and it passes the filepath of the scene to load as a parameter. The SceneButton script connects to its internal Button node to detect the actual button press, and simply emits the pressed signal with the filepath that is configured when the button instance is created. 12345678910111213extends Controlexport(String, FILE, '*.gd') var ScenePath = '&lt;some default file path&gt;'onready var buttonNode = $Panel/MarginContainer/Buttonsignal pressed(pathToScene)func _ready(): buttonNode.connect(\"pressed\", self, \"OnButtonPressed\")func OnButtonPressed(): emit_signal(\"pressed\", ScenePath) Signals in the SceneButton (code edited for brevity) Configuring the SceneButtons at Run TimeWhen the app loads and the game scene is ready, its script loops through the Apps array shown earlier and creates the instances of SceneButton. Each instance is populated with the properties of the scene it represents, and its pressed event is connected to an OnButtonPressed handler in the game. 12345678910extends Nodeonready var SceneButtonGrid = $menuContainer/VBoxContainer/MarginContainer/SceneButtonGridfunc _ready(): for app in Apps: var sceneButton = load(\"res://game/menu/SceneButton.tscn\").instance() sceneButton.Initialise(app.name, app.thumbnail, app.path) sceneButton.connect(\"pressed\", self, \"OnButtonPressed\") SceneButtonGrid.add_child(sceneButton) Instancing the SceneButtons (code edited for brevity) Finally, the OnButtonPressed handler creates an instance of the scene that was passed from the clicked SceneButton, adds the instance to the game’s root node, and removes the Menu scene node. 123456onready var MenuContainer = $menuContainerfunc OnButtonPressed(scenePath): var appInstance = load(scenePath).instance() add_child(appInstance) remove_child(MenuContainer) Handling a SceneButton press (code edited for brevity) The Completed MenuAfter a bit of styling I’m happy with the result. Now I have a basic framework to showcase all the tests in one place that’s easy to extend as new tests are built. The source code is up on a Gist. See all the code on my Github Gist Bon appétit!","link":"/2018/09/21/menu-all-gui/"},{"title":"Preventing Visual Studio Recompiles in UWP","text":"I’ve discovered a circumstance where Visual Studio compiles UWP projects every time. NOTE This is a reworked internal blog I posted at my current employer. I’ve altered entity names to avoid IP issues. The ProblemIn my development team we were being slowed down by unneccessary Visual Studio recompiles of UWP projects. We have several projects in our solution structure with quite a few dependencies between them, so it’s reasonable to expect compiles could take a while if you’re changing things upstream in the dependency chain. However when nothing was being changed, we were finding certain UWP projects were still being recompiled by Visual Studio. Those recompiles were taking significant time (over a minute on my Surface Pro 5), which was disrupting our developer flow and sapping our productivity. Sometimes you just need to iterate fast through a few debugging sessions to get a task done, and “edit and continue” isn’t possible in all those scenarios. ResearchThe first place developers look to solve problems like “Visual Studio builds when nothing has changed” is Google. There’s a lot of developers in the world, and it’s likely someone has seen and solved this type of problem before. A few hits were returned when I searched, but nothing to solve our particular flavour of this issue. One hit did provide a means of diagnosis and potentially solving the problem myself, so I started following that. Kudos to this external article: Why Visual Studio keeps rebuilding my projects for no good reason AnalysisThe key takeaway from the article is to turn the build output verbosity up to diagnostic levels and examine the output. When you do this, the output window is flooded with information from MSBuild showing exactly what it’s doing during a build. By examining such output you get a real sense of how much complexity is managed by modern compilers on our behalf, and you gain an appreciation of how much it’s doing for you. The key clue to what’s causing the problem is in the first few lines of the output of each project build. In my case it was the following line: 1Project 'In-house.Framework' is not up to date. Copy to output file 'C:\\dev\\SolutionFolder\\ProjectFolder\\Source\\In-house.Framework\\bin\\x64\\Debug\\Themes\\Generic.xaml' missing from output location. The build expected a xaml file to be present and it wasn’t. If I did 2 builds consecutively this line persisted, so clearly Visual Studio needed some help to get this file output. The reflex for any seasoned developer seeing this message is to find the source file and set its Copy to Output Directory property value to Copy if newer. This means that if there is no file in the bin folder already, or the file is present but is an older version at build time, Visual Studio should copy the file there. That way if a developer changes the file during development it will be compiled into the resulting application correctly. When I looked at this particular file I discovered that property was already set to Copy if newer. For experimentation I tried setting the property to Copy always and rebuilding but Visual Studio never actually output the file. Forceful MeasuresTaking matters into my own hands I copied the file there myself to see if it had any effect. Building now produced a different message: 1Project 'In-house.Framework' is not up to date. Input file 'C:\\dev\\SolutionFolder\\ProjectFolder\\Source\\In-house.Framework\\In-house.Framework.csproj' is modified after output file ''. This is an interesting message, and I wasn’t entirely sure what it meant as it didn’t match the change I had just made. It seems as though somewhere in the complexity of the generated build process, MSBuild determined the project file was changed so it forced a project rebuild. This rebuild was what I expected so I thought I was making progress. Once that build had finished I rebuilt without changing anything and was met with the following message: 1Project 'In-house.Framework' is not up to date. Project item 'C:\\dev\\SolutionFolder\\ProjectFolder\\Source\\In-house.Framework\\Themes\\Generic.xaml' has 'Copy to Output Directory' attribute set to 'Copy always'. It appears even though Visual Studio doesn’t actually copy the xaml file, it still checks the Copy to Output Directory property value to see if it needs to force a project rebuild, and does so when the value indicates it should. Bad BehaviourIt appears that for xaml files at least, the Copy to Output Directory property is no longer useful. In conjunction with the messages from the MSBuild output, this property is now a source of confusion. MSBuild expects a xaml file at build time and the usual means of control to ensure the file is present at compile time simply don’t work. What’s worse is those usual means of control actually force a rebuild, which is the action you’re trying to prevent in the first place. Given the manipulation of the Copy to Output Directory property is a common practice by developers to help Visual Studio build correctly, this is potentially a cause of common productivity loss among all UWP developers. Theory about Root CauseI don’t have the means of digging into Visual Studio’s code or MSBuild to find exactly why this happens. From experimenting with the presence or absence of xaml files in the bin folder, different Copy to Output Directory property values, and many, many rebuilds I have developed a theory about what is going on. In UWP, the xaml files are no longer used in the build process that requires them to be present in the bin folder. At some point in the past, binary representations of the xaml files (the xbf format) began being built and output for use in XAML applications, rather than the plain text xaml files. It is these xbf files that are important to be present in the bin folder at compile time, not xaml files. The MSBuild messages are leading the developer astray. I suspect that when the binary format feature was being implemented by Microsoft, an oversight or deprioritised task (or two) meant the messages from MSBuild weren’t updated, and Visual Studio wasn’t updated to properly support the new feature. The result is a perfect storm causing a bad consequence, and the impact is widespread productivity loss for UWP developers. The Actual FixEmpowered by the knowledge I had accumulated through investigation and the theory about the root cause, we can finally address the rebuild problem. The fix is simply to tell Visual Studio not to copy the xaml files at all (set the Copy to Output Directory property to Do not copy) and allow the xbf feature to do what it’s supposed to do. Once in this configuration everything works as it should; changes to xaml files are correctly detected by Visual Studio (forcing a recomple), xaml files without changes are correctly skipped (no recompilation), and cleaning the solution (via Visual Studio or by removing the bin and obj folders from the file system by some means) and building correctly forces a recompile. Of course, you will need to find all the xaml files in your projects that have unwanted Copy to Output Directory property values, because each will cause unwanted builds. This is easily done by opening each of your UWP projects in a text editor (like VS Code) and searching for (and removing) CopyToOutputDirectory elements on any xaml Page elements. ConclusionSetting the Copy to Output Directory property to anything but a value of Do not copy on xaml files is now considered a bad decision. Doing so will cause unnecessary compilation of your UWP projects, which will hamper your productivity as a UWP developer. I hope the various teams at Microsoft can come together to address both the misleading messages from MSBuild and the Visual Studio behaviour around the Copy to Output Directory property on xaml files at build time. An improvement could be to provide a compiler warning when the value is not set to Do not copy; this could improve developer awareness that setting these properties now has unintended and (likely) harmful consequences. In any case, fixing both of these issues will squash yet another cause of poor productivity for UWP developers, and that would be a good thing.","link":"/2018/11/10/prevent-vs-recompile-uwp/"},{"title":"Building a Pause Menu - Part 2","text":"That pause menu I’d finished building? I gave it super-powers. Untitled photo from an article about the Living With Elephants Foundation PREVIOUS POST IN THIS SERIES Building a Pause Menu - Part 1 - The first version BackgroundIf you’ve been following this blog you would have sensed my relief at the end of the last post when I had finally established a Pause Menu in my Capabilities app. The Pause Menu unlocks my ability to navigate between the various screens without needing to restart. It validates my design choice of laying out screen selection buttons in a grid, which effortlessly allows me to add new screens with navigation already in place. You would also have read my discontent with Android for not providing a way for apps to shut themselves down. This is a restriction imposed on my design I had not anticipated, and discovering it at the end of Pause Menu development both surprised and frustrated me. You can see my unhappiness expressed in those final paragraphs. The Elephant in the RoomFollowing that post I intended to move onto building a new screen for the app, developing more of the features I need for the game design I have planned. I also intended to write shorter posts in a more timely manner, hopefully making them easier to write and less gruelling to read in the process. That’s what I was hinting at with the last line of the post: I’ll be back in the new year with more posts about my adventures in Godot development. See you then! When I wrote those words I was looking forward to shaking it up a little, with a new screen to build and a fresh blogging approach to try. Life HappensHere we are an entire year later and clearly things didn’t unfold as expected. Numerous external factors de-railed my plans and periodically swept me further away from where I wanted to be. Throughout the year I was frequently disappointed with my slow progress and inability to blog updates. I had to keep reminding myself how grateful I am to have no fixed end date for this project; an approach which circumvents the Crunch Culture plaguing the rest of the gaming industry. So while I frequently felt deflated at my lack of apparent progress, at least I wasn’t crushing my mental health into oblivion trying to meet impossible deadlines, or compromising my design by cutting features, or canning my project altogether. REFLECTION Even experienced developers have to give themselves time to adapt when adopting new technology. What I’m trying to achieve in Godot without any modern gaming engine training or experience, with a vision that people describe as “overly ambitious”, tells me I need to give myself more space to learn and discover than I’ve done so far on this project. Baby StepsWhile there were times during the year when I was unable to do any work on the project, my resolve to continue held without wavering. I may not have been achieving anywhere near the pace I wanted but I remained passionate about forging ahead. That’s the secret to getting anything done; no matter what happens just keep moving forward. So What Have I Done This Year?Needless to say I overcame numerous and occasionally treacherous obstacles. On reflection I’ve improved my Godot skills a long way and have achieved more than a glance at the final outcome provides. I do wish I would learn to be more lenient with myself in the present when I’m stressed about my perceived lack of progress! It feels fantastic to finally be sharing the progress I made this year. With so much to cover I’ve decided to jump right to the chase and show you the completed outcome of the year’s work. Here’s a video showing the new and improved Pause Menu I’ve built. There’s still plenty of room for juice but the mechanics are all in place. Have a watch and I’ll explain a few things on the other side. The final outcome: Pause Menu rewrite 2019! With touch being the primary input mechanism for the game I’m building, I decided to embrace that fact and invent a selection mechanism I’ve never seen before. This led me on a journey to create a procedural generation system for the UI, allowing me to compensate for different form factors in the various platforms and devices I’m targeting. I plan to cover the design philosophies, decision making, system development, and problem solving in a series of posts that extend this topic as a series. But that is for another time. I hope you enjoyed this glimpse of what I achieved in 2019. I look forward to posting more soon. PREVIOUS POST IN THIS SERIES Building a Pause Menu - Part 1 - The first version NEXT POST IN THIS SERIES Building a Pause Menu - Part 3 - Designing the new version","link":"/2019/12/27/building-a-pause-menu-part-2/"},{"title":"Building a Pause Menu - Part 4","text":"The new design is a lock. Time to prototype. Wonderspaces by Israel Palacio PREVIOUS POSTS IN THIS SERIES Building a Pause Menu - Part 1 Building a Pause Menu - Part 2 Building a Pause Menu - Part 3 - - - The first version The rewrite showcase Designing the new version Skip Preface and Jump to the Post PREFACE The bulk of this article was written during the height of the first wave of the COVID-19 pandemic in Australia (where I live). Australia chose the lockdown method as our response to dealing with the virus. You would think that would mean I had more time to devote to my blog, but the contrary is true. With the added chaos of having a partner also working from home, two daughters being schooled from home, restrictions on supermarket purchases and shopping for my elderly mother resulting in almost daily trips to the shop, I actually had less time to work on this post than I usually do. Further, as I prepared the video accompanying this post, I discovered the free software I normally use had started begun driving people to purchase the product by putting watermarks on the video exports. As a software developer myself I understand and don’t object to this practice, however I felt their pricing didn’t reflect the quality of the tool and couldn’t justify spending so much on something I use so little. Seeking both paid and free alternatives, I now use Blender for video editing software. While I haven’t made anything large or complicated yet, I feel I work quicker with it than with my previous software. It’s more difficult to use and doesn’t have all the features I currently need though, so time will tell. At least there’s a lot of online content demonstrating how to use and configure it. I’m happy with the results so far. Hopefully it improves my throughput in the long run. As I write this COVID-19 continues to devastate many corners of the world. Hundreds of thousands of people have died as a result of this virus. Wherever you are in the world, I hope you, your family, and your friends are doing everything to stay safe at this difficult time. BackgroundIn my last post I went into detail about why I redesigned the Pause Menu for my Capability Test app, the design process, and what the final design was. The Capability Test app is a project where I experiment with code to test and create the building blocks for a game I’ve designed and will be building with the Godot engine. I mentioned how important it is for the Pause Menu to reflect the feel of my game, as I wanted to lift the code and place it into my game once I begin building it. I also stated that designing was only the first step, and I would need to prototype the design to ensure it actually works before moving onto play-testing with other people. In this post I talk about building that prototype. NOTE If you have seen my previous post you’ll have seen a video of the completed product. For the purpose of this post, you’ll need to imagine that all I had at this point was the completed new design, the understanding that Godot has built-in support for move-and-slide, and that I was expecting this to be a relatively simple coding exercise to complete. First stepsAs a reminder, here is the sketch of the design I was going to prototype: Prototyping this would be relatively simple given the majority of the touch interface code had been built in a previous capability test. All I needed to do was to create a few walls, a Thumbnail utilising Godot’s collision system, hook up input gestures to move the Thumbnail, and apply Godot’s move-and-slide functionality to the Thumbnail. I created a new scene for the new Pause Menu because I didn’t want to modify the code in the existing Pause Menu until I had confirmed the new design worked. I typically use this development approach because it ensures I focus purely on the code I’m developing; iterating fast, testing and tweaking until things are working to my satisfaction. Looking at the Godot documentation I found in order to utilise move-and-slide the Thumbnail needed to be a KinematicBody2D and the walls should be StaticBody2D. My scene’s root node was of type Node and I added the Thumbnail node and two Wall nodes. I used a CollisionShape2Das the collision shape on the Thumbnail, and a Polygon2D to create visual of the Thumb using a PoolVector2Array. Both were defined using the properties panel. I made the visual a solid white colour for now. I added a script to the Thumbnail node and pulled in my movement management code. At this stage I included both touch and mouse input because I wanted to explore how accurate Godot’s collision system was by providing precise input, as well as getting a feel for manipulating the Thumbnail with touch. Similar to the Thumbnail node, I added CollisionShape2D and Polygon2D properties to define the collision and visual of the walls (again in white). I set the walls up carefully so the data defining them formed a perfectly angled corner into which the Thumbnail would fit. At this stage I was calculating coordinates and and inputting the data by hand just to get started. Finally I added a node to draw debug visualisations. I wanted to visualise the drag vector as an arrow so I could see the origination point, size, and direction in which the Thumbnail was being dragged. For this I just made a Node2D with a script that exposed an array property called vectors expecting dictionary elements { vector, position }. I then implemented the _draw() method such that, on every frame, each element in vectors would be drawn as an arrow with a filled circle at its position and a length and direction defined by its vector. I updated the Thumbnail script to capture a reference to this debugVisualiser node using onready, then cleared and add the gesture vector while the gesture interaction was occurring. It took a bit of jiggery-pokery to get the code working like I wanted (I’m no GDScript professional quite yet!), but before long I had the prototype up and running. I had to pinch myself to believe it, but the move and slide behaviour worked exactly as described right out of the box! Here’s a video showing the prototype of the scene at this point. I’m moving the Thumbnail around with the mouse and have ensured the pointer is visible in the video for reference. You can observe when I’m click-and-dragging by the visualisation of the gesture vector as a purple arrow, and see how the Thumbnail slides along the walls right into the corner and stops. Prototyping with Godot’s move-and-slide It Works! Or Does it?On the surface things appeared to be working perfectly. But as all good engineers know, you have to put your model under stress to ensure there aren’t any edge cases skulking about. Unfortunately for me, when I pushing my prototype to the limit I noticed a couple of such issues. First, I discovered that when the gesture vector became sufficiently large, the Thumbnail would push right through the wall and appear on the other side. This isn’t normally an issue for Godot as the move-and-slide feature is typically used in platformer games where the velocity of objects is small enough and the problem never arises. However in my case I’m targeting a variety of devices with different form factors, so the gesture vector very easily reaches these large amounts. I experimented with different wall sizes to see if I could prevent the issue. This is when I noticed the second issue; when the gesture vector was sufficiently large and the wall was sufficiently thick, the Thumbnail would slowly intrude into the wall. Issue showcase. I moved the wall visual back from the CollisionShape2D to highlight the intrusion effect. For my design I want the walls to form an impervious boundary for the Thumbnail, not be soft and squishy like memory foam. Building a Bespoke Move and SlideMove and Slide isn’t a particularly difficult problem to solve, it’s just a bit of vector geometry. There’s plenty of documented examples of how it works on the web. Personally I think this KinematicBody2D Collisions Done Right page from Kids Can Code explains it best. Here’s a reference diagram from that post: Because I couldn’t get Godot’s implementation working for me with my own use case, my only way forward would be to build my own implementation. Vector geometry isn’t difficult to codify, so why not? Because I was restricting my walls to be convex shapes by design, there would be a maximum of 2 collisions to detect. This didn’t seem like too much work for GDScript so it should be performant enough. I simplified my model so that I could focus on the math, starting by determining when a gesture vector positioned at the centre of the wall geometry would collide with a wall edge. After several failed attempts at trying to work this out in my head, I decided I needed to visualise what was going on. I extended my debugVisualiser node to draw lines (with circle terminators), points, and text. It took a while to get the debugVisualiser working just right, but it was worth the effort. With interactions now visible it wasn’t long before I has successfully coded the math to detect when the gesture vector collided with one of the walls. Visualising the point where the gesture crosses a wall boundary ASIDE I have always wanted to build a mathematical visualisation tool like this, and I was excited to finally have built one. The solution was so elegant, and having achieved it made me feel like a boss. I’m so proud of myself, I’m still excited to have such a simple and versatile tool in my kit! On a RollWith the collision point detected, I could now focus on building the slide. It was a simple matter of vector math to calculate the vector from the collision point to the end of the gesture vector, and then use the normal of the wall to project back onto the wall to calculate the slide vector. Here’s a video showing what these vectors in orange (again, utilising my debugVisualiser): Visualising the slide for the first collision The next step was to calculate the extent of the slide; the vector from the collision point on the wall to the end of the wall (line). In my use case, because I only using convex shapes for the wall/thumbnail, this is the point of the algorithm where I can stop calculating. The Thumbnail cannot slide any further than this extent as it is in a corner. In terms of the Pause Menu interface, a Thumbnail in a corner means it is selecting the option positioned at that corner. As such, in the following video, I show the word CONNECTED and the slide vector at this point in yellow. With all this in place, I can start testing how swiping gestures feel. Towards the end of the video you’ll see me confirming swipe left moves into the top-left corner and swipe down moves into the bottom corner. Visualising the slide extent and the Connected state Wrapping it all upWith the simplest case resolved I set about generalising my algorithm to work for any wall, and cleaning up. The visualisations came in very handy to confirm when things were working and when they weren’t. Here’s a final video showing the algorithm working for all three walls of a triangular boundary. Visualising all the walls and connections My bespoke move-and-slide algorithm was almost complete. All that remained was to account for the width of the Thumbnail so that it didn’t move past the wall. At first I started trying to figure out the logic on the fly using the distance from the center of the Thumbnail along the gesture vector. Then I realised I could do an up-front calculation and move the logical boundary of the walls in by a certain amount from the visible boundary get the same effect. This avoided complicating the already-working algorithm. Calculating the amount to bring the logical boundary in was just a little more vector math and I was done. I now had everything I needed to build the new Pause Menu. I’ll cover that in my next post. PREVIOUS POSTS IN THIS SERIES Building a Pause Menu - Part 1 Building a Pause Menu - Part 2 Building a Pause Menu - Part 3 - - - The first version The rewrite showcase Designing the new version","link":"/2020/04/25/building-a-pause-menu-part-4/"},{"title":"Changes of a Cosmic Scale","text":"I’ve reached a critical juncture in the development of my game and engine. I’ve been developing a game for the past 15 months. It’s been a labour of love, as any indie game developer can attest to. However, recently something happened that has forced me to pivot my roadmap significantly. Before I get to what’s happened I’d better give you some context, because I realise it is sorely lacking at this point. A Brief History of the ProjectOver two years ago I discovered how I could make the time to pursue my dream of developing (and releasing) a game. I’ve wanted to do this my whole life, and after several uncommitted attempts and a gap of at least 20 years since my last try I realised the passion to do this was burning as intensely as ever. What’s more, taking on a side project was going to save my sanity, as my 22+ year career as a programmer wasn’t letting me practice the skills I felt I needed. I love a good synergy, and here I could cover both issues by building a game. I chose to use the Microsoft .NET stack because the bulk of my developer experience is there, and the devices I owned (phone, desktop, laptop) were all in the Windows ecosystem. My laptop was a 128Gb Surface Pro 1 (bought the day they were available) and my phone a Nokia 930 running Windows 10. Both were in fantastic condition (both operationally and aesthetically) despite almost constant usage. However the Surface was so full of development tools and SDKs there was little free space for my project or other tools I would need. I only had very small daily periods to work on the project; my available time was daily train commutes to and from my day job. This started out as a 20 minute commute in the morning and the same in the evening, however along the way I did move house and this became two 30 minute windows that I currently enjoy. In terms of committing time to side projects this didn’t sound enough at first, and the disbelief I could be productive in such small bursts put me off for a long time. Being unable to find any other time I gave it a go, and two years later I’m amazed at how much I’ve achieved. ASIDE I submitted a talk to present this development process at a developer conference last year but it didn’t get up unfortunately. It’s worth sharing the challenges and benefits of this process perhaps one day I’ll at least blog about it. As a consequence of the short periods of time available and being disconnected from the network while commuting, I chose not to start with an existing game engine as I felt it the learning curve require a heavy investment in online tutorials and other online research to be productive. So I decided to build the game engine on my own from scratch as well as the game itself. I know that sounds insane, but I didn’t rush into this decision blindly; I knew it would be super tough (and more than I could imagine), so I did quite a bit of experimentation and exploratory coding before I decided it really was achievable. I knew I would be happy with this decision as it would finally afford me the practical experiences I was craving (writing high performant, memory efficient code, and exploring previously unvisited areas of the .NET Framework), as well as allowing me to dump decades of mental pseudo-code into reality to see if my theories actually worked. Setting My Own ExpectationsGiven it would take a long time to build both an engine and a game from scratch under normal working circumstances, and that I only had 40 minutes a day (20 minute sprints), I had to be realistic about time. Most game developers and studios set themselves deadline/completion dates for their games, but that just wasn’t applicable for what I was doing and how I was working. I chose to not fix a date, but to fix the quality of the game at a high level and it would take “as long as it takes” to build. This meant I could commit to getting the design of the engine right from day one, focusing on performance, no memory leaks, and no blocking garbage collection when playing. The latter was the strongest in my mind, and I did a lot of further research to confirm what I believed was the best practice to avoid garbage collection in .NET (mostly declaring all memory use up front via object pooling). This lead to some satisfying conversations with people who were actively working on the .NET garbage collector at the time, the discovery of a memory profiling tool (BenchmarkDotNet), and a practice of regularly profiling the .NET framework to ensure my choice of code implementations in the engine code were optimal. I even blogged about that in my first blog post. Finally, I knew that if this worked it would be the beginning of a number of games I would produce. I wasn’t expecting to make any money from this first project (being on the Windows marketplace, which is a pretty small market compared to Apple and Google) so I expected to release the result for free, with some clever ideas for monetisation that didn’t change the game dynamics. The value I would get out of this project would be very personal: Practicing and developing the skills I felt were missing in my current skillset (personal skills growth) Keeping sane at work (not trying to force personal development into my day job as it just wasn’t happening) Getting years of game routines out of my head and into reality (overcoming impostor syndrome) Expanding my knowledge of game development terminology and practices (setting myself up for future game projects) Finding other game developers locally and on the web that I could talk to (establishing a new social network) Finding SupportI didn’t take that last point lightly. I knew what I was undertaking would require endurance as progress would be agonisingly slow. I back myself for mental toughness and sheer determination, but it would be foolish to take on such a project without support from others who knew what I was going through. So I reached out to the only game developer I knew (from a meetup he presented at once), who directed me to a local community called Let’s Make Games. I started attending Playup Perth where local developers were provided space playtesting their games for review and feedback. I exchanged Twitter handles with a few local developers and made a few connections. Social media then quickly expanded my network, and even revealed video content such as the excellent vlog Just Make Game by Armitage Games. Starting with MicrostepsMy first development task was to figure out was which rendering technology to use. I wrote several tests to trial Win2D which proved more than adequate. It works great as a performant renderer (siting astride DirectX) and also provides a game loop controller managing timing complexity for fixed timestep loops (and variable if you want it). It cohabits nicely with the Universal Windows Platform (UWP) which was my target language framework. Each new test I created established further confirmation that my stack integrated comfortably, allowing UWP gestures and touch and .NET sensor support (accelerometers and the like). An early test - UWP gestures with Win2D rendering These initial tests took some time but were important. I needed to be confident I wasn’t committing to a path leading to an impassible road block somewhere along the way. They also allowed to solidify my solution architecture while I explored the technology space. Another early test - testing the performance of sprite batches in Win2D As my tests grew in number and I learned more, the design for my game started to form in my mind. Resisting the urge to fully specify the design up front (who knew what I would discover along the way) I fleshed out about 70% of the design without committing to paper (for design agility). I now had my base game design and practical knowledge of the technology choices I could use to build my way there. My direction was established and it was time to start moving. One of the final tests - parallax clouds working across multiple devices with UWP The Zodman Project (#zodproj)After working on the project for several months my enthusiasm hadn’t waned and I had produced a surprising body of work (at least to me). Inspired by other indie devs in my social network, I decided to begin sharing my progress on social networks as well. At the time I didn’t have a blog, so I decided to use Twitter to give tweet-blogs about my work augmented by video. With a need to relate these tweets together I realised I needed a hashtag. I decided to label all my side projects under a single banner; The Zodman Project. Zodman is the nickname/handle that has stuck with me from my gaming days way back in the 1990s, and I’ve used it prolifically as an ID across the web for all this time (using ZodmanPerth when Zodman is already taken). The #zodproj hashtag would be used to group together all my online posts about my side projects and provide some context to individual posts. As a lover of coherent narrative, I felt it was important to start sharing progress from the beginning of the project, so my tweets started with those initial tests and proceeded in linear order. I wanted to show the body of work so far in weekly tweets until I caught up to where I was coding. I found this difficult because I had to find the time to create the video and also maintain my code so that the older tests continued to function while I coded at the bleeding edge of the project. This was particularly challenging as I continuously refactored my engine as I discovered better ways to do things. One other challenge was the size of a tweet which only allowed 140 at the time. While this was a blessing as it brutally focussed the scope of my posts (and let the video do the talking), I started getting that nagging feeling that maybe I should bite the bullet and start blogging as well. A Blog is BornFollowing a lengthy performance investigation with PerformanceDotNet, and having to augment the way it works to concatenate multiple runs into a single report, I finally had something to share that was beyond what Twitter could suitably accommodate. It was time to get my own blog. After scouring the web for ready-made blog services, worrying about loss of IP/data and control over layout and style, I settled on source controlled static site generation and managing deployment and hosting myself. Hexo is a static site generator that suits my commute-working style. Posts are markdown which I edit using VS Code and suits source control. It includes a local node server for testing the site locally prior to deployment. I then deploy to GitHub pages which provides hosting for a single, free, size-limited static website per account. After paying for a DNS entry and routing requests to Github, my blog was online. I had finally created an environment to share ideas in depth the opportunity to start a game blog. However, I struggled to create the time necessary to create blog posts. It takes some time to create posts, especially when you haven’t blogged in a while and you’ve got a lot of ground to cover. I found I wasn’t blogging because of the time required, and all the time I had available went into developing the things I wanted to blog about; Catch 22. Still, at least I was pushing my game and engine forward so I could be happy with that. I resigned myself to remain uncomfortable with falling behind in sharing my journey until I had some time to reflect on resolving the issue. Upgrading my Development EnvironmentEven though things were going pretty well with development I was starting to really feel the lack of disk space was starting to hamper progress. I had been eyeing off the new Surface Pro (2017 edition, 5th generation) with envy, and when my wife’s laptop finally died it was the perfect time to change up and gift her my Surface Pro 1 (a major step up from her awful laptop at least). I went for the basic I5 with 8Gb RAM and 256Gb disk space. I went for extra disk because of my troubles with space in the past. I expected this device would last me for the next 5 years or more, and the knowledge that in that time I would start using pre-built game engines and other tooling that would eat into the free space. The development experience on the new machine wasn’t too different to the 1st generation to be honest, though the additional screen resolution and CPU power made a noticeable difference. The new screen doesn’t handle being in daylight on the train as well though, and I had to purchase a polarised screen protector because when I was wearing my sunglasses I couldn’t read the screen! The screen brightness isn’t as bright either and I still find it a little difficult to read. I must say though that it’s well worth buying the new Surface Pen and nib set. The tilt control allows you to sketch with a feel almost like using a real pencil and I can’t wait to do some art with it when I can find the time. The Game ChangerAnd then it happened…the big whopper; the event that would change everything. Microsoft had released a Windows 10 update that stopped my phone from working properly. First I noticed that my Twitter app stopped scrolling after displaying about 20 posts. This was annoying as every spare moment I had would be spent looking through my feed to stay motivated by the indie dev network. I was missing out on a lot of great content and motivation. Then I noticed my email wasn’t working to well either. I could no longer email research findings to myself for review on a desktop sometime later. The loss of this functionality made things a bit too much to bear. I had encountered similar issues with a prior phone update, but not for such an extended period. The phone partially installs the new update (causing software quirks) but it fails and tries again at the next opportunity (my settings say to try nightly). I waited several weeks before I decided a successful update may not be coming anytime soon. My phone version was out of support with Microsoft so they were under no obligation to correct the issue. I had no idea when the next update would come, or if it would resolve the issue; there were no guarantees the next update would not have similar issues. The platform I was targeting was now unstable and I was forced to reconsider the approach I was taking. Without a stable platform there was no guarantee that the limited customer base that existed would still be there if I got up and running again and managed to finish my game in reasonable time. I had a difficult choice to make. I was weighing up years of lost learning opportunities through developing the engine myself to move forward on an existing game engine that target a set of devices I didn’t own. While it took some soul searching the way forward was inconvenient yet obvious; I had to change platforms sooner than anticipated and target the Apple and/or Android market. The event was literally a game changer. Not a Total LossAs sad as it was, the change did not mean I had wasted a year of development. Let’s look again at the list of things I wanted to achieve: Practicing and developing the skills I felt were missing in my current skillset (personal skills growth) - SuccessI learned a great deal about .NET Garbage Collection, memory management, object pooling, and using arrays in anger. My most recent refactor of the game engine (which was incomplete at the time where “the event” happened), was further proof to myself that I had an excellent handle on solution architecture and that I knew how to make the choices that would benefit working with and extending the engine. Keeping sane at work (not trying to force personal development into my day job as it just wasn’t happening) - SuccessI was a lot happier at work as I was no longer frustrated that opportunities to learn in areas I felt I was lacking never came. I was managing that progress on my own time, and at work I could focus on doing work to the best of my ability and having more fun. Getting years of game routines out of my head and into reality (overcoming impostor syndrome) - SuccessRoutines such as tile maps, sprite batching, user input without compromising architecture, were amazingly cathartic to get out of my head and seeing in real life. The most enjoyable of all though was writing my own collision detection in pure geometry, not only because maths is awesome but because I love explore areas of mathematics I have never seen before. I had expanded my horizons and in doing so blew away any reason to call myself an impostor. Expanding my knowledge of game development terminology and practices (setting myself up for future game projects) - SuccessBecause I worked as close to the metal as my framework choice allowed without fighting the language framework, I was exposed to the gnarly detail of the areas of game development I felt I needed. When I saw people using existing game engines such as Unity and Unreal and had glimpses of how they were structured, it confirmed to me I was doing things right and learning all the right things. Finding other game developers locally and on the web that I could talk to (establishing a new social network) - SuccessI could never have come this far if I were truly alone. While it’s been a disappointing year for socialising with local indie devs, I’ve developed relationships both in my own country and beyond. Though these relationships are new, they give me confidence that I belong among such a group of inspirational and talented people who understand what I’m going through and can support me when I’m feeling down. The Do-OverOverall my effort over the previous year has born substantial fruit and despite my sadness at the passing of an old friend (Windows Phone as a development platform) it can only be called a success. I now have a new direction to move in with purpose, with the benefit of it being a more densely populated market of people to potentially see my wares. Given I no longer need to write a game engine myself, I anticipate my game will evolve more rapidly as I am liberated to focus solely on that. This should go a long way to making me feel a lot closer to the cadence at which other indie devs work, despite still only having two half-hour sprints a day available to devote. I’m hoping more rapid progress will translate into more rapid sharing of screenshots etc, and in doing so any self doubt about being an impostor will be much more infrequent. I’ve been given a second chance to do things right so it’s important I reflect on the journey so far, reinforce what I’ve learned to myself and to consider how to be more effective this time around. The most obvious place to improve is where I have been feeling the most angst; socialising my work. Doing it BetterThrough my effort with social media so far I have discovered which channels are good for different sizes of communication and the frequency that works best on them. Where I have been previously frustrated by my inability to devote time and been blocked because I hadn’t established a channel, I now have a better understanding on how to do it better this time. This means: TwitterThis channel will be used to report up to the minute stuff without context, because the frequent flow of tweets itself creates the context which readers can follow (pun unintended). It should also be used to participate in community groups such as #screenshotsaturday. I will continue to use #zodproj to flag my game tweets and aim to post more often and only with up-to-date information. Game BlogNow that I have created a space to blog I will use it more frequently. I’ve learned that while it is a good channel when you have more words than can be squeezed into a single tweet, it takes far too long to write posts if you leave it too long. Just as frequent releases of software makes deploying releases easier, I’m believe more frequent posts will make blogging easier. Perhaps whenever I pass a major milestone in my development, or perhaps when I’ve passed a few minor ones, I’ll do a quick post so they don’t take too long to write. I’m looking for a sweet spot between creating my game and blogging about it. I hope that now I’ll be using a pre-built, rich featured game engine I can focus on building the game itself, which will create a more satisfying timeline of progress and I won’t feel guilty about taking my limited time away to work on blog posts. Final thoughtsBecause of the limited time I have available it’s taken me two weeks of actual time going by to create this post. In that time I have purchased, received, and set up a new Android phone, been researching game engines for suitability to my development practices, and started learning about one in particular in more detail. I can see I’ve got a long road of discovery ahead before I get back to being as productive as I was, but I believe this effort will rapidly increase the speed at which I deliver and I will be more creative as a result. I look forward to posting more frequent updates in the future.","link":"/2018/03/30/changing-everything/"},{"title":"Measuring C# Performance with BenchmarkDotNet","text":"I’m measuring optimal C# code implementations for a game engine I’m building. BackgroundI recently took on a “little side project” to pursue a lifelong dream: designing, creating (and releasing!) a computer game. Though I’ve had the desire to do this for more than three and a half decades, I never quite got around to making a consistent, concerted effort. It’s not because I’m lazy, I’ve just managed to pack a whole bunch of experiences into my life so far and, until recently, couldn’t find enough time to devote to the cause. By “little side project” I’m being a sarcastic of course. I knew when I started this journey that building a game is no trivial feat, and there would be unknown unknowns that would arise and need to be dealt with along the way. That discovery was part of the exciting thing for me because I love learning new things. However, because I really enjoy a challenge (and from the necessity of my circumstances), I made the decision to write the game engine as well; from scratch. Fortunately I’d pursued life experiences that included all the ingredients I would need to pull off such a feat from both a creative and practical standpoint, including a lengthy career in software engineering and commercial experience in UX design. I’ll explore the circumstances that drove those initial decisions another time. Right now, it’s enough to know that the game engine is written in C# on UWP using Win2D, and targets Windows tablets and phones that have accelerometer sensors and touch screens. The game is a top-down 2D space shoot ‘em up with a retro arcade look. Build A Game Engine in C#…Are You Crazy?When people talk of building game engines, C# and .NET aren’t terribly popular choices. The .NET framework, while a much loved and carefully crafted platform, hasn’t exactly been built for the demands of a high-performing game engine in mind. I don’t think my engine will be too demanding though, so using .NET is a feasible option. Win2D is a powerful ally here, being a finely tuned wrapper over DirectX that is built for performance. Win2D also provides a game-loop that gracefully handles the complexity of loop timing management, so that’s one less thing I needed to worry about. This leaves the game engine “merely” doing some user input handling, physics, collision detections, game logic, and image rendering. What could go wrong right? 😀 As long as I build with the Garbage Collector in mind, and execute code in tight enough loops, I would hopefully get a smooth 60 frames per second across my target devices. This is a nice engineering challenge that will keep me happily satisfying my inner geek for some time. Performance Front and CentreI was reasonably sure I could make something decent with my technology choices, and a few test applications later I committed to the task ahead. With a freshly read copy of Bob Nystrom’s amazing eBook Game Programming Patterns up my sleeve, years of mentally constructed game systems and designs that were eager to escape my mind, and the skills I had nurtured over my career, I figured I had a decent shot at a solid result. But confidence alone was never going to be enough. Going into development of the engine I knew I would have to make every nanosecond count. While the architecture was wholly under my control and I trusted my skills, there was one concern that still hung heavy on my mind; would the .NET Framework itself be performant enough? There was only one way to know for sure; I was going to have to write performance tests for the framework itself. And when you want to know the performance of .NET code I know of nothing more suited to the task than BenchmarkDotNet. BenchmarkDotNet and LINQPadI’m a heavy LINQPad user from way back. Among other things, LINQPad enables me to test hypotheses quickly away from the complexity of my larger code bases. Once I’m happy, I move the validated code into the main codebase and do some manual integration tests. This process works really well for me, so when I found myself needing to test different C# implementation patterns in isolation, LINQPad was a natural choice. Fortunately for me my timing was just right; BenchmarkDotNet had just added support for running inside LINQPad, and LINQPad had just added support for hosting BenchmarkDotNet. While the road since has been a little bumpy, the state of both tools was good enough to provide what I needed. Testing Different Iterator Patterns in .NETI really enjoyed writing my own collision detection system; it’s the kind of challenge I relish. My starting point was an excellent tutorial by Nilson Suoto called Collision Detection for Solid Objects, and after a lot of experimentation I had a really tight system. I had a Sort and Sweep service using cheap AABB checks to find collections of objects with potential collisions (the broad phase), which were used to discover the actual collisions using Minkowski Sums and Convex Hulls of object geometries (the narrow phase). We’re not covering the details of the implementation here, but you should know the narrow phase contains expensive CPU operations and I’ve spent an appropriate amount of time optimising the hell out of them. Codewise, the Collision Detection system drives the Sort and Sweep service using an iterator pattern. You probably know this pattern from implementing IEnumerable&lt;T&gt;; GetEnumerator() methods that yield appropriate results. This is how I implemented it at first, and it made for some nice, readable code. However, I had a hunch that this generic interface may not be the most performant way to drive the service, so I devised a plan to test different implementations of the iterator pattern to see if I could find time savings. I also had concerns that the use of generics would add pressure to the .NET Garbage Collector, and as such I was interested in verifying this measuring memory aspects of each implementation as well. Initial BenchmarksSwitching to LINQPad, I wrote up a Worker class to simulate the Collision Detection system doing expensive work by summing the values yielded by a Driver class, which represented the Sort And Sweep service. Main() simply called the benchmark runner to run with the Worker type. I marked up the Worker with a MemoryDiagnoser to report on memory consumption, and because of my cross device intentions I thought it’s prudent to explore 32bit and 64bit generated code. The code looked a little like this: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546[MemoryDiagnoser][RyuJitX64Job, LegacyJitX86Job]public class Worker{ [Benchmark] public void NormalEnumerator() { Driver driver = new Driver(); int count = 0; int sum = 0; foreach (int i in driver) { sum += i; count++; } }}class Driver : IEnumerable&lt;int&gt;{ const int _size = 10; int[] _array; public Driver() { _array = new int[_size]; for (int i = 0; i &lt; _size; i++) _array[i] = 1; } public IEnumerator&lt;int&gt; GetEnumerator() { for (int i = 0; i &lt; _size; i++) yield return _array[i]; } IEnumerator IEnumerable.GetEnumerator() { return GetEnumerator(); }}void Main(){ BenchmarkRunner.Run&lt;Worker&gt;();} As well as the usual output appearing in LINQPad’s results window, running this produced a nice set of reports in various file formats in the subfolder \\BenchmarkDotNet.Artifacts\\results where LINQPad was executing. NOTE You will probably need to run LINQPad as an admin to get it to work. Here’s an extract from a report running this code: From this I was able to reason a few things about a single implementation of an iterator pattern on a small sample size of 10 items. This was a good start and served as proof that the concept of measuring performance in this way worked. Many Rivers to CrossThinking ahead, I knew I had challenges to face. First, an array size of 10 probably wasn’t representative of how big the object arrays would end up in my game. I wasn’t sure how many objects I would need, but I knew performance rarely scales linearly. I wanted to test the same implementations of iterator patterns on array sizes with various orders of magnitude. Unfortunately BenchmarkDotNet didn’t support marking up benchmark methods with injectable parameters (array size in this case), so I had to roll my own solution. Second, while I knew I could have different implementations inside a single Worker, I would need to be careful not to let the sharing of code scope compromise what I was measuring; the implementations could not affect one another. Third, the way I simulated actual work in the implementations had to correctly represent code in my original code base that hadn’t been written yet! Without attention to this detail it would be easy for my abstraction to stray from what would be reality, influencing the implementation and not representing a realistic scenario. Fourth, the implementation code should be reasonably easy to read. I would need to discover where the line was between performance and readability while I thought of innovative implementations. Fifth, I wanted to explore iterators over different kinds of objects such as value types, reference types, and generic types. The result would guide the object structure forming the basis of my collision detection system. Finally, I wanted all these tests to run on a single execution run and roll into a single report table. I figured I would use Excel for final analysis so it would be best if the results were automatically collected together so I didn’t have to do any manual amalgamations every time I completed a test run and wanted explored the results. The Usual Black-Box ChallengesI very quickly discovered that when I performed multiple tests over a single Worker type within the same run, the naming convention BenchmarkDotNet used to create the filenames would result in the overwrite of previous reports. I created a helper class to create backups of report files on disk and track the generated file names. I then added amalgamation of the final report using these backup files using AngleSharp (an open source HTML parser) to rip apart the tables in these files and create my own HTML report. Along the way I also discovered that LINQPad’s Dump() methods weren’t available when BenchmarkDotNet was running, and would cause a terminating exception. This hampered my ability to verify that iterator implementations were working properly (i.e. actually iterating), so I added a “runmode” state to the helper class that allowed me to switch between manual verification of code (where code is executed using LINQPad and not via BenchmarkDotNet) and producing the benchmark reports (where BenchmarkDotNet controlled the execution and not LINQPad). The “runmode” also let me avoid throwing exceptions by isolating calls to Dump() behind conditional checks. Finally, I created a state machine that looped through the various implementations, changing the size of the array each time. The state of the machine was another addition to the helper class. The report backup system hooked into this state to generate unique report names, which were used in turn to create the column values in the amalgamated report. Different Implementations to TestI thought of 4 different iterator implementations to try out: NormalEnumeratorThis was the IEnumerable&lt;T&gt; implementation I already had using GetEnumerator() methods and yield to return results at appropriate points. HandCrankedEnumeratorThis implementation is a POCO with Reset() and MoveNext() methods and an EOF property for controlling the loop. Would removing a broad, generic implementation improve performance and reduce memory usage? HandCrankedWithMoveNextStateThis a variation of the previous implementation that eliminated the EOF and returned feedback from Reset() and MoveNext(). This reduced the number of statements required to drive the iterator which I suspected would translate into better performance. ResultSetFullyCalculatedThis avoided iterating altogether, replacing it with a big synchronous operation calculating all the results in one pass and exposing a Results array. Obviously this would have higher memory requirements, but would it perform better? NOTE My game engine runs on a single thread. Multi-threaded concerns such as the timing of the read and write of values from different threads did not need to be considered. Here’s a gist containing a copy of the final LINQPad code: Amalgamated ReportOnce I was sure all the implementations were working correctly I switched on all the benchmarking options I wanted, plugged in some array sizes, and let it run. On my Surface Pro with an i5 processor this took about 42 minutes, but was worth the wait because the data is rich and dense. Here’s the amalgamated report from one such run. If you’re used to the usual output columns BenchmarkDotNet produces you’ll recognise where I’ve inserted the other relevant data. The Type and Size columns correspond to the type of Worker (IntegerWorker for value type tests, ItemWorker for reference type tests, and GenericWorkder for generic type tests), and the size of the array used to produce the result. The other columns come straight from the BenchmarkDotNet reports. Analysing with ExcelWith all the data is in one place I wanted to use Excel’s analysis tools to help me compare the results and derive conclusions. Copy and paste as text did the trick, though I did have to strip the alphabetical characters from the numeric columns and change their cell format to a number. Once that was done I used a red-green colour scale conditional formatting on the numeric data to create a performance heat map, and used table filters and sorting to do the analysis. Here’s the same amalgamated report formatted this way in Excel: Looking at the Mean column there was an obvious candidate for being too slow; the red cells. Turns out the NormalEnumerator method (the implementation I was already using) was the worst performing, taking almost twice as long as the other methods. I filtered that method out and dug deeper. When I looked at the fasted results in each Size group (those with the greenest Mean values), I could see that the ResultSetFullyCalculated method looked suspiciously slow. I checked each ResultSetFullyCalculated method against the other methods in the same Size group by eye and confirmed it; calculating the results in one hit was slower than flowing back and forth between the Worker and the Driver. ASIDE This was a pleasant surprise. I had expected a higher memory footprint and a lower execution time. This kind of result is exactly why you need to validate early; it’s much cheaper to build things right the “first time” instead of trying to refactor once you’ve built a whole bunch of stuff on top of a false assumption. I had a real Adam Savage moment with this finding. Science! Let’s filter out the ResultSetFullyCalculated method and keep digging. Now we get down to the pointy end of the analysis. With only two methods remaining to compare against each other, there’s nothing much to do but check each pair individually. The results are really close, and difficult to compare due to the variance in Error and StdDev. However the HandCrankedEnumerator is marginally faster in almost every pair. This was a bit of a surprise given I had assumed less statements in the caller would mean less execution time, but without checking the final generated code you can never be sure how such assumptions pan out. The performance difference isn’t significant enough for me to take my investigation that far, so I’ll happily settle on the variant that’s easier to read, maintain, and is generally faster. How Bad is Your Memory?Did you notice I haven’t talked at all about the memory usage by any of the implementations? Well, throughout my analysis I was giving them the occasional, cursory glance. The fact is, there wasn’t anything significant to worry about. There was only very small Gen 0 in all implementations apart from NormalEnumerator, which was always the highest memory user in each group and always puts the most pressure on the Garbage Collector. The general rule of thumb I’ll be taking forward is that for balancing performance and memory concerns, always go with a POCO. Of course, when it counts I’ll still be testing assumptions of course! SummaryIn the end, because of my findings, I refactored my Sort and Sweep service to be a POCO with Reset() and MoveNext() methods and an EOF property for controlling the loop as per the HandCrankedEnumerator test implementation. It really didn’t take long to convert it over, and I now feel confident and satisfied that through my due diligence I’ve squeezed a little more performance out of a crucial game subsystem. Onward and upward!","link":"/2017/12/10/measuring-csharp-perf-with-benchmarkdotnet/"},{"title":"Saying G&#39;day to Godot","text":"I’ve taken my first steps to developing a game on the Godot 3 game engine. Shrimp on the BBQ - 1984 Australian Tourism TV Ad, modified with Godot Logo BackgroundFollowing a thorough selection process outlined in my last post I selected Godot 3 as the game engine I would build a game with. The game is a top down 2D shooter, and much of the design had been created previously as I was developing it on an engine I was building myself at the same time. Things didn’t work out with the OS I was targeting, and I’m now planning to build it with Godot 3 for mobile and tablet devices on the Android and (probably) iOS platforms. Check out my previous posts which cover the journey so far in more detail. INFO How does one pronounce “Godot”? I’ve seen and heard a lot of discussion this topic. The game engine is named after the play “Waiting for Godot”, whose admirers also have strong sentiment about correct pronunciation of the word. I couldn’t find any conclusive answers, so I’m going with the first pronunciation I heard; the “g” sound from the word “green” followed by a “doh” sound as in the “dough”. I very, very slightly pronounce the first “o” as well but it’s barely noticable. In fact, it sounds almost the same way we Australians pronounce the word “G’day”, hence the name of this blog post. What to Build First?Given I’m effectively back where I was when I started on my own engine, it makes sense to rebuild the same capability tests I had to ensure Godot was able to accommodate the game features I have in mind. As well as helping me to learn Godot, it’s also an opportunity to compare the performance of Godot over my own engine by implementing the same tests. I expect to find Godot performs much faster than my own engine as it’s much closer to the metal than I was working previously, but it will be great to see this in practice. I chose to port my touch manipulation capability test first. Touch input is the area I have the most concern with in Godot, and my test should be relatively simple to port. My game uses touch and gestures for controls, and the touch manipulation capability test allows me to explore the touch features in Godot. The test is simple; three coloured boxes that can me moved by a drag gesture, scaled by pinch and zoom, and rotated with a twist of two or more touch points. The boxes are selectable, so once selected you don’t have to be touching the object to continue manipulating it. Here’s a video of the completed test on my former engine (running on Windows Phone) to show how this works: Touch manipulation capability test on my former engine Coding with GodotWith Godot there are several language choices, including C# (which I have most of my experience in). However I chose to start with Godot’s own scripting language; GDScript. This language was developed specifically for Godot by their core development team and I felt it would be much easier for Godot to convert my intentions into native code on my choice of target platforms if I used it. Besides, new Godot features would probably appear in GDScript first and I’d prefer they be available for me if I need them. There was also the opportunity to use external editors for coding. My favourite code editor is VS Code and it already had multiple extensions for working with GDScript. After running with both the Godot editor and VS Code for a while, I found the convenience of working inside the Godot editor far outweighed the features the editor lacks (touch scrolling the code window and multi-line editing for example) and I went all in with the built in editor. When creating objects in Godot you define a “scene” which is a composition of a hierarchy of “nodes”, each with it’s own purpose. It took me a while to find the right nodes to use for this test, and after a while I realised I was implementing my objects at the wrong level. I was misled by the word “scene” and thought it was synonymous with “view”, and while you probably can work this way it’s not how Godot is designed to work. A scene is more synonymous with “class” or “entity”, and a view is just a scene that composes multiple other scenes together. Nodes are like characteristics or behaviours of the scene and it’s the combination of composing nodes into a scene that defines the object. Nodes can contain any number of other nodes, and nodes can be instances of scenes. This allows the kind of encapsulation and inheritance heirarchies you would expect from an object oriented language, so all the OO coding patterns and techniques still apply. It was relatively easy to become familiar with the way scenes and nodes work together to form larger pieces, and I quickly became confident with dealing with problems as they arose. Of course, I still needed to familiarise myself with what all the built in nodes were for, so I continued to watch youTube tutorials to see how other people solved the problems I would be coming across as I build out my game. Input with GodotGodot has a comprehensive input event system where input events are propagated through all nodes in a scene on every cycle of the game loop. Various events on nodes are fired until the event is marked as handled or passes all the way through, allowing several ways to accommodate different input processing methods. As well as raw input information, a built in input map system allows you define maps from various input devices (keyboards, joysticks, mice, console controllers, etc). Input maps abstract the controller away from the event handlers and simplifies your code. Godot’s documentation; as comprehensive as the InputEvent itself In my capability test, touch input was my focus. While simple touch gestures allow “click” and “double-click” control out of the box, more complex manipulations need to be constructed from raw “screen touch” and “screen drag” events. This was in contrast to where my original test was written, as UWP has a built-in manipulations class that supports all sorts of gestures. While creating custom gesture handling can be frustrating, it can also be beneficial. Even though I lost some time developing the gestures I needed, I know I’m not having to employ a bloated built in node containing gestures I don’t need. Further, as more and more people adopt touch devices and Godot 3, there’s opportunity to share community-built solutions such as this to speed up the development process for everyone. It’s worth mentioning that while I did search for and find some pre-existing touch handling code for Godot, it wasn’t in a format I preferred and I wasn’t sure if it was performant. I decided to use writing my own handler as a step to learning to code in Godot and use existing material for reference to help solve my problem. Manipulations in GodotIn the previous version of my capability test on UWP I found the delta events of manipulations worked very well when consuming the events. Delta translation, rotation, and scale values are easily applied to matrix transforms on objects, so my touchController (as I called it) would definitely expose these events in signals somehow. To get to this point I would need to process the raw InputEventScreenTouch and InputEventScreenDrag events from Godot and apply some logic to convert into my manipulations into signal data. I started with simple touchStarted and touchStopped signals that would fire whenever a finger started touching and stopped touching the screen. This essentially echoed the Godot InputEventScreenTouch event’s pressed property, but allowed me to start creating a touchInput scene with the controller signals connected up to a debug label for testing. Selectable BoxesNext I needed to draw the coloured boxes to be manipulated on the screen. Godot doesn’t have any vector shape nodes that I could find, so I ended up creating a colourBox scene with a custom draw method to draw squares with borders. I draw two squares on top of the other (first with the border colour, then a smaller one with the fill colour) which ensures the border looks crisp and doesn’t antialias/bleed when the box is scaled. I added some variables for the border and fill colour, and for the size of the box and the width of the border. Then I created a generic function to create instances of the 3 boxes in my scene (red, green, and blue). Now I wanted to make the boxes selectable. I was expecting the built-in input system would provide a way to determine the top-most object under a point on the screen but it appeared not to be so. After discussing the issue on the Godot Discord and toying with a few approaches it seemed the best approach in my circumstances was to allow the boxes themselves to set some state if they were touched and have a controller in the parent scene determine which shape should be selected if multiple hits were reported. While this felt a little clunky and over-specified at first, it did lend itself to developing a selectionController that isolated the selection of boxes from the rest of the parent scene to hide most of the clutter. I considered going a step further and creating a behaviour node that would be added to boxes when they are registered with the selectionController to encapsulate the input logic, but I don’t know if I’d ever reuse that code so I’ve left it as is for now. All that remained is for the selectionController to work out which node is selected by looping through the registered boxes, examining whether they were being touched, and assigning to a variable for use in the main scene. Translation ManipulationNow that I had selectable objects it was time to create the manipulations I needed. I went for the easiest one first; the translation. From my experiment with creating touchStarted and touchStopped signals I knew that each touch point would be assigned it’s own ID (called “index”), and those indexes are created using an increasing integer. When a touch starts I add the index, origin, and position to a dictionary of touch points for tracking. I also record the index of the highest touch point seen, which I use later to form the manipulations loop. During InputEventScreenDrag events from Godot I update the position of the stored touch points in the tracking dictionary. If the event drag is on the point with the highest index I call a manipulation handler, forming the manipulation loop. When touch stops (the InputEventScreenTouch event with event.pressed equal to false), I remove the point from the tracking dictionary (and update the highest index if required). 123456789101112131415161718192021222324252627282930313233func _input(event): if event is InputEventScreenTouch: if event.pressed: # add point to register var tp = { \"index\": event.index, \"origin\": event.position, \"position\": event.position } touchPoints[event.index] = tp touchPointCount = touchPoints.size() # remember highest index for synchronising manipulation signals maxIndex = event.index # signals emit_signal(\"touchStarted\") else: # remove points lastTouchPoints.erase(event.index) touchPoints.erase(event.index) # update maxIndex if necessary touchPointCount = touchPoints.size() if maxIndex == event.index: if touchPointCount != 0: maxIndex = touchPoints.keys()[touchPointCount - 1] else: maxIndex = -1 # signals emit_signal(\"touchStopped\") elif event is InputEventScreenDrag: # update position of this touch point var tp = touchPoints[event.index] tp.position = event.position # Start manipulation loop if all points have been synchronised if event.index == maxIndex: handleManipulation() All that remained was to calculate the delta translation (vector) using the change in position of one or many touch points. For each touch point in the tracking dictionary, the vector from its current position to its position on the previous pass through the manipulation loop are added together. The result is divided by the number of touch points to calculate the average value. This is the translation delta. 1234567891011121314151617func handleManipulation() : # Calculate translation delta var deltaPosition = Vector2(0, 0) if lastTouchPoints.size() != 0: var lastManipulationCentre = Vector2(0, 0) # the last centre of all touch points var thisManipulationCentre = Vector2(0, 0) # the new centre of all touch points var tpCount = 0 for tpKey in touchPoints: if lastTouchPoints.has(tpKey): tpCount += 1 lastManipulationCentre += lastTouchPoints[tpKey].position thisManipulationCentre += touchPoints[tpKey].position # calculate the last and current manipulation point lastManipulationCentre /= tpCount thisManipulationCentre /= tpCount # calculate delta position deltaPosition = thisManipulationCentre - lastManipulationCentre Once calculated I simply emit a manipulationChanged signal from the controller containing a delta with a translation property containing this translation delta value. In the parent scene I consume this signal by simply adding the delta.translation to the selected node’s position and the “drag” manipulation is complete. 1234567# continuation of `func handleManipulation()`# signalemit_signal(\"manipulationChanged\", { \"delta\": { \"position\": deltaPosition } }) Rotation ManipulationNext I tackled rotation manipulation. The logic for this is different because to create rotation you need (at least) two touch points, and use the angle between them to calculate the delta. If there are more than two touch points, the angle between each neighbouring pair of touch points is added together and averaged. I use another tracking dictionary, keyed by the indexes of the point pair, to compare the change in angle between subsequent passes of the manipulation loop. 123456789101112131415161718192021222324252627282930313233# inside `func handleManipulation()`# calculate angles between touch point pairs (if required)var deltaAngle = 0var touchPointCount = touchPoints.size()touchPairs.clear()if touchPointCount &gt; 1: var angle var thisTotalAngle = 0 var lastTotalAngle = 0 var pairCount = 0 var pairKey var lastPair var thisPoint var nextPoint = touchPoints.values()[0] for i in range(1, touchPointCount): thisPoint = nextPoint nextPoint = touchPoints.values()[i] pairKey = Vector2(thisPoint.index, nextPoint.index) # calculate touch pair data angle = fposmod(baseVector.angle_to(nextPoint.position - thisPoint.position), 2 * PI) touchPairs[pairKey] = { angle = angle } # check if pair is relevant (was present in last pass) if lastTouchPairs.has(pairKey): lastPair = lastTouchPairs[pairKey] pairCount += 1 # get angle details for calculating delta angle thisTotalAngle += angle lastTotalAngle += lastPair.angle # calculate delta angle if pairCount != 0: deltaAngle = (thisTotalAngle - lastTotalAngle) / pairCount This angle delta is inserted into the delta object in the manipulationChanged signal, and consumed by adding the value to the selected node’s rotation property. This completes the implementation of this manipulation. 12345678# inside `func handleManipulation()`# signalemit_signal(\"manipulationChanged\", { \"delta\": { \"position\": deltaPosition, \"angle\": deltaAngle } }) COEXISTING MANIPULATIONS Note how the logic to calculate the translation and rotation work independently on the same group of touch points. This means you can put all five digits of your hand on the screen (for example), and both drag and rotate at the same time. This produces the effect you would intuitively expect and feels very natural. Scale ManipulationThe final manipulation is often called “pinch and zoom” and in my sample project it is used to scale the size of a box up and down. The logic is a lot like the logic for rotation and shares a lot of the code; it requires (at least) two touch points, and the change in distance between each neighbouring pair is averaged to produce the scale delta. The same tracking dictionary is used to track this distance for each point pair between passes of the manipulation loop. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# inside `func handleManipulation()`; merging scale with angle calculation# calculate distances and angles between touch point pairs (if required)var deltaAngle = 0var deltaScale = 0var touchPointCount = touchPoints.size()touchPairs.clear()if touchPointCount &gt; 1: var angle var thisTotalAngle = 0 var lastTotalAngle = 0 var pairCount = 0 var distance var thisTotalDistance = 0 var lastTotalDistance = 0 var pairKey var lastPair var thisPoint var nextPoint = touchPoints.values()[0] for i in range(1, touchPointCount): thisPoint = nextPoint nextPoint = touchPoints.values()[i] pairKey = Vector2(thisPoint.index, nextPoint.index) # calculate touch pair data angle = fposmod(baseVector.angle_to(nextPoint.position - thisPoint.position), 2 * PI) distance = thisPoint.position.distance_to(nextPoint.position) touchPairs[pairKey] = { angle = angle, distance = distance } # check if pair is relevant (was present in last pass) if lastTouchPairs.has(pairKey): lastPair = lastTouchPairs[pairKey] pairCount += 1 # get angle details for calculating delta angle thisTotalAngle += angle lastTotalAngle += lastPair.angle # add distance details for calculating delta scale thisTotalDistance += distance lastTotalDistance += lastPair.distance # calculate delta angle if pairCount != 0: deltaAngle = (thisTotalAngle - lastTotalAngle) / pairCount # calculate delta scale if lastTotalDistance != 0: deltaScale = (thisTotalDistance - lastTotalDistance) / lastTotalDistance The scale delta value is inserted into the delta object in the manipulationChanged signal. Consuming it is slightly different, as this value is multiplied by the selected node’s scale property to produce the intuitive affect. 1234567# signalemit_signal(\"manipulationChanged\", { \"delta\": { \"position\": deltaPosition, \"scale\": deltaScale, \"angle\": deltaAngle } COEXISTING MANIPULATIONS Note again that the way this manipulation works does not interfere with the other two manipulations, and produces an intuitive, natural way to manipulate objects using the same touch points. Manipulations and InertiaIn the original demo I could flick boxes around manipulations whereas in my Godot version I cannot. In UWP, the built in manipulation system had an inertia feature that I chose not to implement in my Godot version. I don’t require this feature for my current game, however if I ever need to add inertia manipulations I think it would be relatively trivial to add the logic to my touchController. Ship it!My touch manipulation capability test was now complete. While things worked great running this test on my Surface, I really wanted to see it on mobile (at least on Android). I wanted to ensure that when Godot translated to a different set of binaries the behaviour of the manipulations wouldn’t change. At the time I was using Godot 3.0 and setting up to deploy to Android was a pretty convoluted and painful. This was mostly due to having to install tooling and configuration outside Godot, as the config inside Godot itself was relatively easy. Since then a lot has happened, with some changes with the way the Google app store treats Godot applications, and decisions about how Godot supports Android exports, and I’m happy to say there appears to be plans to simplify things a lot for Godot 3.1 (or at least in a near-future version). Once Godot (and my development machine, and my Android phone) were correctly configured, exporting to Android wsa a breeze. When I connect my phone to my Surface, an option appears in Godot to deploy to Android at the press of a button. A few brief moments later and my capability test was running on my Android phone and working in every way it did on my Surface. You beauty! See all the code on my Github Gist Final thoughtsWhile it was initially disappointing that I had to build my own manipulation system in Godot, I’m satisfied with the result. The performance of this capability test, at least measured through my own observation, appears on par or better than the previous version. I’ve deployed a Godot app to an Android device, and everything works the way it should, which was a relief. I’m happy that things are progressing and I can move on to the next task. Here’s a video showing the new capability test running on Godot. Touch manipulation capability test on Godot","link":"/2018/08/21/say-gday-to-godot/"},{"title":"Building a Pause Menu - Part 1","text":"I built a pause menu to improve the experience on my Android phone. BackgroundAfter constructing a home menu for my Capability Test App (see my last post) I thought I was done with menus for a while. However the experience of closing and re-opening the app just to change tests quickly became tedious. I wanted to address this pain before adding more. The time was right to build a Pause Menu providing in-app navigation. This also allowed me to explore how Godot handles game pausing (a feature I’ll use in the future). It was an opportunity to kill two birds with one stone. “Please don’t kill us; we’re magnificent creatures!” The Godot Pause MechanismThe Godot engine has a simple mechanism for pausing a game. Flipping the get_tree().paused boolean to true at run-time sends your game into pause mode. In pause mode your input handlers and process loop code are no longer called by default, stopping your game from running. Then you configure the nodes you want to continue processing during pause mode. In my case this meant configuring a Pause Menu scene that contains controls to unpause the game, go back home (i.e. to the Main Menu), and to quit the app. You can add whatever features you want to run while the game is paused. It’s a very simple and effective system. Evolving the Solution ArchitectureIt may be easy to build a Pause Menu UI and hook up to Godot’s pause mechanism, but to perform the navigation back to the Main Menu I needed to evolve my architecture. Previously the only navigation in the app was launching a new scene when selected from the Main Menu. At the time it was appropriate for the Main Scene script to handle instancing and launching of the selected test scene. With the introduction of a Pause Menu, navigation back to the Main Menu was now required. This meant rethinking how scene creation, activation, and navigation was handled throughout the app. To prevent the navigation code spreading throughout the codebase as the app grew, I decided to replace existing navigation code with a centralised service. After considering various ways to achieve this I created a gameStateManager node to be this service. This node would be responsible for scene navigation via a SetActiveScene() function. It would also track the state of the app (GamePaused or PlayingScene) and expose a command system to allow navigation to the various fixed game scenes and states (PauseGame, GoHome, QuitApp, ContinueGame). NAMING CONVENTIONS My personal preference is to use the term “Manager” instead of “Controller” when I name such objects. When I talk about “Manager” objects it’s likely your own understanding of a “Service” or an MVC “Controller” object accurately describes my intention here. After a first pass at writing the gameStateManager I found my code was too messy. I took this code smell to indicate my gameStateManager was doing too much and needed breaking apart. I created two child nodes; an activeSceneManager responsible for changing the active scene, and a pauseSceneManager responsible for managing the Pause Menu and its operations. The gameStateManager remained the central point for processing game commands and tracking the game state. This breakdown keeps each node focused on a single area of responsibility (the ‘S’ in SOLID design principles). NOTE At this point in learning Godot I’m finding it difficult to apply object-oriented design principles (such as SOLID) to the node system. Nodes seem to behave more like methods on an object than as objects themselves, which feels a little unnatural. However my instincts about when to break script files apart still trigger in a timely fashion. I’m getting more of a feel for how and when to listen to my instincts while working in Godot, and I’m building my personal development rhythm for organically growing a GDScript codebase. Using Signals as a Messaging SystemThere are two types of messages my navigation system needs to work. First I have game scenes that need to change the active scene by messaging the activeSceneManager. Second I have a Pause Menu that needs to send game commands to the gameStateManager for processing. All my game scenes (the Main Menu, Pause Menu, and capability test scenes) are created by the Manager nodes. This provides an opportunity to connect signals from game scenes to the Manager nodes at the point of creation. As long as these signals implement standard message contracts, a one-way message channel is formed from the game scene to the appropriate Manager node along which messages can be passed. The ‘SetActiveScene’ Message Channel (and Contract)If a game scene (such as my Main Menu) has a requirement to navigate to a new scene, it must implement a signal I’ve called SetActiveScene(args) . The contract for args is a dictionary containing a key of either sceneInstance or scenePath, and an optional key sceneName. An existing node instance or path must be passed as the value. The sceneName value is only used for debug output. The signal is connected to the SetActiveScene(args) function in the activeSceneManager. When a node instance or path is passed in the args, the activeSceneManager creates the new scene instance (or use the passed one), sets the new scene instance as the active one, and connects the SetActiveScene(args) signal of this new instance to itself. The new node is added as a child of the activeSceneManager itself. The previously active scene child node is removed (if there is one) and signal connections are disconnected. Thus when the currently active scene needs to navigate to a new scene, it calls emit_signal on SetActiveScene with appropriately message args and the navigation is performed by the activeSceneManager. 12345678910111213141516171819202122232425262728var _activeSceneInstance[...]func SetActiveScene(args): var nodeInstance if (args.has(\"sceneInstance\")): nodeInstance = args.sceneInstance elif (args.has(\"scenePath\")): nodeInstance = load(args.scenePath).instance() else: return if (args.has(\"sceneName\")): print([\"Playing Scene\", args.sceneName]) RemoveActiveScene() _activeSceneInstance = nodeInstance _activeSceneInstance.connect(\"SetActiveScene\", self, \"SetActiveScene\") self.add_child(nodeInstance)func RemoveActiveScene(): if (_activeSceneInstance == null): return _activeSceneInstance.disconnect(\"SetActiveScene\", self, \"SetActiveScene\") remove_child(_activeSceneInstance) _activeSceneInstance = null Managing the active scene and connecting the SetActiveScene signal in the activeSceneManager The ‘ExecuteGameCommand’ Message Channel (and Contract)The game command message channel is implemented differently. Currently in my Capability Test app the only node required to send game commands is the Pause Menu scene. Because the Pause Menu’s UI maps directly to game commands I can hardwire signal connections from the Pause Menu scene directly to the gameStateManager when it is created by the pauseSceneManager. To do this I created MenuSelected(), QuitSelected() and PlaySelected() signals on the Pause Menu scene. These are emitted by the Pause Menu scene when the user selects the Menu, Quit or Play buttons on the Pause Menu. These signals are connected to an ExecuteGameCommand(command) function on the gameStateManager by the pauseSceneManager as the game boots. In this case the message (the command parameter) is a simple enum value. 1enum gameCommand { PauseGame, GoHome, QuitApp, ContinueGame } When the pauseSceneManager creates the Pause Menu it connects its signals using a hardcoded message value. 1234567var _pauseSceneInstancefunc Initialise(gameStateManager, pauseScenePath): _pauseSceneInstance = load(pauseScenePath).instance() _pauseSceneInstance.connect(\"MenuSelected\", gameStateManager, \"ExecuteGameCommand\", [ gameCommand.GoHome ]) _pauseSceneInstance.connect(\"QuitSelected\", gameStateManager, \"ExecuteGameCommand\", [ gameCommand.QuitApp ]) _pauseSceneInstance.connect(\"PlaySelected\", gameStateManager, \"ExecuteGameCommand\", [ gameCommand.ContinueGame ]) Creating the game command message channel from pause menu to the gameStateManager The ExecuteGameCommand(command) function on the gameStateManager simply checks which command was passed and performs the desired actions. It either updates the game state and shows/hides its manager node children, or quits the app. 12345678910111213141516171819202122func ExecuteGameCommand(command): match command: gameCommand.GoHome: SetGameState(gameState.PlayingScene) $activeSceneManager.Home(); $pauseSceneManager.Hide() gameCommand.PauseGame: SetGameState(gameState.GamePaused) $pauseSceneManager.Show() gameCommand.ContinueGame: SetGameState(gameState.PlayingScene) $pauseSceneManager.Hide() gameCommand.QuitApp: get_tree().quit()func SetGameState(state): match state: gameState.GamePaused: get_tree().paused = true gameState.PlayingScene: get_tree().paused = false _currentGameState = state The gameStateManager being responsible for changing state and executing game commands The BootstrapWhen the app first starts I need to do some preparation before the app is ready to be consumed by the user. The environment in which the app runs needs to be configured, and the initial game scenes and nodes required need to be created and instanced. When this preparation is done using code it is called a Bootstrap, and is an example of the Bootstrapping design pattern. The code entry point of my app is the _ready() function in the the game scene script. This is where my bootstrap begins. Its first task is to configure the run-time environment for the app, which is performed using Godot engine properties and commands: The mouse pointer is hidden. My app is controlled purely with touch (even on Windows desktop) I configure Godot not to quit automatically. I will manage when the app closes using a Main Menu. I configure Godot not to quit when Back is pressed. This setting is used when the game is running on Android. NOTE A consequence of configuring quit behaviour is the app must handle OS events. I’ll talk about that later in this post. 1234# Set up game environmentInput.set_mouse_mode(Input.MOUSE_MODE_HIDDEN)get_tree().set_auto_accept_quit(false) # Must be false to allow pause menu to work on Androidget_tree().set_quit_on_go_back(false) # Must be false to allow pause menu to work on Android The second task is to initialise the game scenes and nodes. Rather than hardcoding or hardwiring scenes with the editor, all my game scenes (the Main Menu, Pause Menu, and capability test scenes) are defined using data. When the app first starts I want the Main Menu to show and have the Pause Menu ready. I pass the paths of these scenes to the gameStateManager for creating, instancing, and adding to the game scene’s tree using a custom function called Initialise. 1$gameStateManager.Initialise(\"res://game/menu/Main.tscn\", \"res://pauseMenu/PauseMenu.tscn\") The gameStateManager initialises its child Manager nodes activeSceneManager and pauseSceneManager by passing relevant scene paths using more custom Initialise functions. Once those complete the initial game state is set to gamestate.PlayingScene. 1234func Initialise(homeScenePath, pauseScenePath): $activeSceneManager.Initialise(homeScenePath) $pauseSceneManager.Initialise(pauseScenePath) SetGameState(gameState.PlayingScene) Initialise on activeSceneManager creates an instance of the Main Menu using the passed homeScenePath. It keeps a reference to this instance for later (navigation back to the Main Menu), and sets it as the active scene. This adds the Main Menu instance as a child of activeSceneManager (itself). Because activeSceneManager manages the app’s active scene, this is achieved using only local function calls (SetActiveScene() via Home()). 12345678910var _homeSceneInstance;func Initialise(homeScenePath): _homeSceneInstance = load(homeScenePath).instance() Home()[...]func Home(): SetActiveScene({ sceneInstance = _homeSceneInstance, sceneName = \"Home\"}) Initialise on pauseSceneManager creates an instance of the Pause Menu using the passed pauseScenePath. It creates an instance of the Pause Menu, wires up the ExecuteGameCommand message signals to the gameStateManager, hides the instance, and adds it as a child of pauseSceneManager (itself). 123456789101112func Initialise(pauseScenePath): var gameStateManager = self.get_parent() _pauseSceneInstance = load(pauseScenePath).instance() _pauseSceneInstance.connect(\"MenuSelected\", gameStateManager, \"ExecuteGameCommand\", [ gameCommand.GoHome ]) _pauseSceneInstance.connect(\"QuitSelected\", gameStateManager, \"ExecuteGameCommand\", [ gameCommand.QuitApp ]) _pauseSceneInstance.connect(\"PlaySelected\", gameStateManager, \"ExecuteGameCommand\", [ gameCommand.ContinueGame ]) _pauseSceneInstance.hide() self.add_child(_pauseSceneInstance)func Hide(): _pauseSceneInstance.hide() NOTE Because pauseSceneManager is a sibling lower in the game scene tree than activeSceneManager, the Pause Menu will appear on top of the active scene. The background of the Pause Menu is partially transparent to allow the active scene to be visible underneath when the Pause Menu is shown. Godot System NotificationsI configured Godot to ignore quit requests because I want to manage how the app closes myself. I want the Pause Menu to appear when the app window loses focus on desktop environments or the back button is pressed on Android. This is achieved by handling system notifications emitted by Godot. Godot’s notification system uses event-like function calls to notify different parts of the engine when system events occur. Godot games and apps can also choose to be notified and respond to these events by implementing a _notification(what) function in any script(s). Each notification has a unique ID to identify what event occurred, and each ID is exposed as a constant in the MainLoop class to facilitate responding appropriately. In my case I want the gameStateManager to show the Pause Menu when the app window loses focus (MainLoop.NOTIFICATION_WM_FOCUS_OUT) or the back button is pressed (MainLoop.NOTIFICATION_WM_GO_BACK_REQUEST). I also want the app to terminate when the app desktop window is closed or the Quit option is selected on the Pause Menu (MainLoop.NOTIFICATION_WM_QUIT_REQUEST). By implementing a _notification() function in the gameStateManager script I can use local function calls to ExecuteGameCommand() and pass the relevant gameCommand value to achieve the desired effect. 12345678func _notification(what): match (what): MainLoop.NOTIFICATION_WM_FOCUS_OUT: ExecuteGameCommand(gameCommand.PauseGame) MainLoop.NOTIFICATION_WM_QUIT_REQUEST: ExecuteGameCommand(gameCommand.QuitApp) MainLoop.NOTIFICATION_WM_GO_BACK_REQUEST: ExecuteGameCommand(gameCommand.PauseGame) Now the game reacts appropriately to operating system events, shows the Pause Menu and terminates when I want it to… except… Godot Games Cannot Terminate Themselves on AndroidOn Android systems a call to get_tree().quit() doesn’t terminate the app. Android appears to be designed this way, as if all applications running on it are services that should always remain open. Currently it requires the user to force the application closed by using Android’s overview (square) button. There seems to be valid ways to force close Android apps in Java, so I have created a feature request to allow Godot to support force closing in the future. Moving Navigation from the Main Menu ScriptWith the Pause Menu hooked up all that remains is to remove the old navigation code used by the Main Menu to launch the various test scenes and replace them with the SetActiveScene message channel. As previously described this is a simple matter of implementing the SetActiveScene(args) signal in the Main Menu script and emitting it with correctly populated message arguments. The connection of the signal is done for the Main Menu scene during its creation by the activeSceneManager. Within the Main Menu scene the buttons are connected to a single OnButtonPressed function in the script. When a button is pressed it passes the details of the test scene selected. Where OnButtonPressed used to create and instance the test scene when called, it now simply emits the SetActiveScene signal and constructs a message using the details passed in. The activeSceneManager does the heavy lifting and looks after the game state. 12func OnButtonPressed(scenePath, sceneName): emit_signal(\"SetActiveScene\", { scenePath = scenePath, sceneName = sceneName }) Ready to RollNow everything is in place and we are ready to run the application. Here’s a video of the application running on Windows (a desktop scenario): The Pause Menu on Windows Desktop Here’s a video of the game running on Android phone. The Pause Menu on Android Phone Designing to Improve the Android ExperienceHere’s a video showing the issue where Android doesn’t quit to highlight why it’s an issue. Godot 3 can’t quit on Android! Given I can’t force the app to close on Android at the moment, the only option I have is to employ clever design choices to counteract this woeful experience. I’ve reflected on this issue for some time. The best options I’ve come up with so far are to either change the button label from Quit to Close (or Minimise), and/or add another label to the Pause Menu explaining that you have to manually close down the app. I would only want these changes to appear when running on Android to avoid breaking the current design for other platforms. Further research is needed to discover how to achieve this, though I suspect adding an argument when exporting to Android will be the first thing to investigate. In any case I’m not happy with either of these options from an aesthetic point of view. I like the symmetry of the current design with short four-letter words to describe intent. I also like the current icons that accurately describe what the intention of the button is by mimicking real-world hardware devices. Changing these to accommodate a functional shortfall on Android weakens the current design to the point at which I’m forced to consider a complete redesign for the Pause Menu. I’m deferring that line of thinking for another time. The best outcome for me is that Godot will support force closing on Android in the future. Given I’m not writing a releasable game just yet (I’m still testing Godot’s capabilities) I can afford to wait to see what happens. Right now I have a menu that works for my current use case and I’ve implemented a nice navigation system that will stand up to future expansion of my app. The code from this article is up on Github as a Gist. See all the code on my Github Gist I’ll be back in the new year with more posts about my adventures in Godot development. See you then! NEXT POST IN THIS SERIES Building a Pause Menu - Part 2 - The rewrite showcase","link":"/2018/12/30/building-a-pause-menu-part-1/"},{"title":"Building a Pause Menu - Part 3","text":"Let’s talk about how I designed the new Pause Menu. LED light trail and trees by Guillaume de Germain PREVIOUS POSTS IN THIS SERIES Building a Pause Menu - Part 1 Building a Pause Menu - Part 2 - - The first version The rewrite showcase Skip Preface and Jump to the Post PREFACE I was around seven when the first home computers and arcade machines unleashed electronic games into the world. I was instantly seduced, and wondered over how to design such incredible experiences. With the appearance of every new reality, that allure was nurtured and became deep-seeded passion. I saw them as a gateway to unrestricted expression and infinite creativity. From the first brush of their magic I burned to be a creator of such marvels; to liberate the unfathomable visions of my mind and cast them into the world for others to enjoy. I spent decades playing them, studying them, absorbing everything they offer, analysing how they do what they do, and dreaming of what I want them to do. Meanwhile I forged a career in software development, learning and practicing the art of code. Despite living in the world’s most isolated city, where good opportunities are rare and difficult to obtain, I have been fortunate and made a successful living as a software developer. I’ve seen many types of developers over the years, and observed how swathes of programmers shy away from creating beautiful visual experiences. Something about the challenges of code grinding consumes many a developer, who turn their backs on the visualisation and interaction aspects of software. They seem content to limit their craft to writing and maintaining systems of pure data. It seems to be an event horizon in the field of computer science, resulting in a majority of people who shun, rather than embrace, the skillset required to fashion creations for the experience of others. I’m definitely not of that mindset. As a highly visual person I’ve always held experiencing software in the same high regard as creating it. I have a huge passion for crafting delightful and seamless interactions, and have eagerly filled the roles left void by engineers more afraid of UI than inspired. At other times I’ve been fortunate to spend time working under and alongside people who specialise in design instead of programming. I wasn’t even aware design was a thing for over a decade of my career, when I finally landed a job on a project requiring the expertise of a full-time designer. The shock of realising there was an entire industry I had never encountered was another world-expanding moment in my journey; like living a lifetime in the moonlight before seeing world in the full sun. I had found the door to a part of me I’d felt was missing but couldn’t describe, and threw myself into exploring the domains of design with insatiable thirst and relentless energy. For several years I did everything in my power to learn all I could about design, the practices of visual designers, and the fields of computer-human interaction. While I still feel incomplete after discovering design so late in my career, I am blessed to have had the opportunity to discover and empower myself with it at all. I am at a point in life where I have achieved a comfortable balance of design and engineering knowledge and experience. I feel no fear as I design and develop. I dream and I do. BackgroundIt may simply be an inability to find the right community, but I rarely see people sharing their thoughts about designing user experiences for software. Sure there’s a plethora of articles, texts, courses, and advice about creating experiences within Web Browsers (Steve Krug has the best book about it I’ve ever found on the topic), yet the vast majority of them seem biased toward the Web as the visual medium. I’m talking about design at a higher level; where are the references on how to think about design for getting the best experience for users of software in any electronic medium? I caught my first glimpse in 2010 when Bill Buxton did the keynote at Microsoft’s Mix10 conference, and his book on Sketching User Experiences ushered me down the path to explore the field of UX (much appreciated Bill). I was beaten over the head with it by Bret Victor talking about Inventing on Principle, and Bret’s work on designing better interactions continues to be the finest I’ve encountered. His current project is pushing boundaries, lifting the concept of software to the physical world. If you’ve never seen Inventing on Principle, do yourself a favour and watch it now! The people and works I’ve mentioned have altered the course of my life in significant and positive ways. When they crossed my path I felt I had uncovered the tip of a giant iceberg; that I would dive in and drink the knowledge until my mind overflowed. But that never happened. I haven’t found anyone since who talks about the subject, let alone expands on it. I’ve had to make do learning from masters of more restricted disciplines, eking whatever I could and pocketing it within my own pool of knowledge. I feel it’s about time I began to write about how I do design and weave it into the narrative of my posts. With that in mind, let’s explore the process I went through to design the new Pause Menu I’ve just built for my Capability Test app. How It BeganAs I’ve mentioned in previous posts, I wasn’t at peace with the first Pause Menu I had created. Sure it was functional enough, allowing me to navigate to all corners of the app. I avoided digging too deeply as to why I was uncomfortable with it and had decided it was good enough for now. I was going to move on and build my next capability test. Not long after making this decision I found myself on a long-haul jet flight for a family holiday. It was one of those rare flights where I hadn’t brought my Surface with me to work on my game, as our destination didn’t have a safe and I couldn’t afford to replace my hardware if something untoward were to happen. Instead, I had brought along some paper and a pencil with which to sketch. I figured if the in-flight entertainment wasn’t interesting I could always practice drawing with physical tools for a change. Sure enough the idea of sitting passively watching TV paled in comparison actively doing something, and it wasn’t long before I pulled out my tools and started scribbling away. Warmup SketchesTo warm up I did a mindfulness exercise to ground myself within the noisy, unfamiliar environment I was in. I find this technique calms my mind and creates a healthy mental space to fill with creative thoughts. Following that I began thinking of what I could sketch. In the past I had sketched enemies for my game and figured that was a good place to start. After a short period of “writer’s block” I was growing impatient so I began scouring my surroundings for inspiration. The seat in front of me had an interesting surround for the phone recharging port and I was inspired by that for a while. The first sketch felt a little too Minion-like for my taste, so I played with softening the square edges and reshaping the head to remove the inference. Here’s the result: Having warmed up I looked around for further inspiration. By utilising things from the real world I hoped my creations would infuse a familiar feel with my own style to make them unique and interesting. After some time I realised the symbols used by the aircraft signage were a potential gold mine of familiar sources and I settled on using an arrow from the emergency exit sign for next piece. The dimensions of the arrow evoked echoes of isometric geometry in me so I sketched this as a potential enemy ship: As a designer I’m always keen to explore playing with negative space. For my next sketch I decided to take the person symbol from the toilet sign as a base and doodle with it. A Sphinx-like figure began to emerge. In a purposeful flurry I began adding symmetry, wings, and really went for it with the use of negative space. The result is some sort of statue-y thing. I don’t have a use for it right now but who knows if it will find some purpose later on in development. Here’s the final image, and the same image rotated so you can make out the person symbol in it’s correct orientation: With my passion for symmetry ignited and my creativity fully warmed up, I wanted to create something deliberate and original. Taking the lead from the head of the person symbol (oh I do love a circle!), I sketched a sphere and began putting some structure around it. This opened up to more play with negative space and I added expression lines to form the impression of a larger circle. On it’s own it looked too controlled, so I added some horizontal lines above and below to indicate some mysterious purpose. I coloured in some of the structural pieces to break the symmetry and it really looked like there was a message trying to be told here. I assure you there isn’t! Succumbing to Design ImpulsesThroughout the time I was sketching these I kept having to ward off distracting thoughts about my discomfort with current state of the design of the Pause Menu. Now that I had sketched a few things and felt I was being productive, and with plenty of flight time remaining, I turned my attention to analysing why I felt uncomfortable with the original design. As a reminder, here’s what the Pause Menu looked like at that time: Three comfortable buttons, with labels and icons. Anyone who paused the game could look at these and know how to use it in an instant. What was it that disturbed me in this simple interface? As a designer I usually strive for such simplicity; for the user to instantly understand what to do with minimal explanation or guidance. The arrangement I had produced could already be viewed as a pinnacle of design; 3 clear choices, a simple tap and away you go. However that design didn’t work on the Android platform. While ideally there are only 3 choices, on Android the app can’t close itself without breaking the development rules of the operating system. I shared my disappointment at discovering this late in development in part 1 of this series. This is knowledge I wanted to carry with me into a redesign. Something else to consider was the distinctive feel of the game. A simple menu such as this would look at home just about anywhere on any app ever. From a different viewpoint however, any one app with this design looks the same as every other app. While that may be fine for my Capability App with an intended audience of just me, I want to eventually lift the code for this menu and put it in my game at a later point, and everyone else would see and use it. When I asked myself “Are you happy with this being the Pause Menu in your game?”, the answer was a resounding “No!” because it just felt too generic. The design of this menu looks straight out of the “form-based business application development” handbook, not “designing for awesome game experiences - masters edition”. It’s the design equivalent of having developer or placeholder art in the final game. This was a valuable realisation to have before I redesigned. One final thing to consider was that I may need to add more options to the menu as development progresses. For example, a player may want to tweak game settings such as the volume level. A familiar place people look to find such options is the Pause Menu, as the menu itself is generally quick to access during game play. The point for me here, however, was to ensure the design is flexible enough to accommodate more options in the future, with minimal development effort, if I need them. NOTE It’s a common usability pattern for players to quickly alternate between tweaking settings and playing the game, trying to find optimal values before continuing with extended gameplay. Designing for this experience means carefully balancing accessibility, speed, and affordance. Because the Pause Menu itself needs to be accessible at pace, there is a natural synergy to providing access to in-game settings from the Pause Menu. Identifying the Root Cause of the ProblemI now had a revised set of requirements to take into a redesign brainstorming session; a Brief. I didn’t write these down at the time because having them swimming around in my mind is usually enough for me. However I’ll list them out for you here so it’s clear exactly what they were. The menu must have 3 choices for navigation: Continue playing Go back to the main menu Quit the app On Android, the Quit instruction doesn’t make sense on it’s own. One of the following must be true on Android: The Quit instruction must not infer the app will close (relabelling?) The Quit instruction could have further instruction on how to actually close the app (intermediate dialog?) The Quit instruction could be removed on Android (avoidance?) The menu could have more than 3 choices in the future The menu could have different choices in the future (e.g. relabelling Quit on Android) The menu should be quick to access The menu should be easy to use The menu should be intuitive to use The menu must be efficient to use (i.e. selection must be fast) Menu interaction should reflect the feel of the game NOTE Notice how I use terms such as must, should and could to phrase my requirements. From experience I have learned the MoSCoW method is the best way to externally convey clear understanding and priority of what is required. I find it beneficial to think that way as well. That last requirement may seem vague because I haven’t blogged about the feel of the game yet. That isn’t to say it hasn’t been defined; I have spent many hours refining my ideas about it. I am yet to create a formal Game Design Document because I enjoy leaving flexibility to discover new ideas while I explore how to code up the building blocks for the game. I’m experienced enough to recognise scope creep most of the time, so that isn’t a concern for me. I have strong ideas about how the game interface will work, how it will look, how it will feel, and how it will sound. For the moment I leave these loosely connected in my mind, swimming around like a soup in my head. Through practice and experimentation I’ve discovered the best way to allow my designer-mind to work is if I formalise such things at the last possible moment, maximising the time I have to work on the problems on my plate today. What I will share for now is that a key principle of my game is the interface must be a delightful haptic experience. Controlling the player ship through touch manipulation is as important an aspect contributing to an enjoyable player experience as anything else in the game, and I’ve designed this principle into the game from the very beginning. With that in mind, when I looked at the first Pause Menu design with it’s three comfortable buttons, it became pretty clear to me what the problem was. There was simply no fun in using that interface. There is no playfulness, no delight. It did not reflect the haptic principle that is a pivotal part of the experience of playing my game. It is a completely forgettable interaction for a user to tap one of those buttons, and that was the crux of the problem. Ideation of the New DesignAt last I understood what it was I didn’t like about the design of that menu, why I didn’t like it, and all the requirements I needed for improving it. Now all I had to do was brainstorm ideas that covered the new Brief as much as possible. Fortunately I still had plenty of time to think while my flight continued, and I already had my creative juices flowing. I was ready to tackle the task. If I truly wanted to celebrate the touch aspect of the interface, I somehow needed to incorporate dragging across the surface with touch gestures. If I was to have gestures I needed to have visual feedback so a player could see the effects of their gestures. If it was to be intuitive it needed to be familiar, even while I create something that uniquely reflects the feel of my game. I began to reflect on established UI controls, testing whether they matched my Brief. Of all the controls, the best fit was the humble ScrollBar. Though restricted to a single dimension, it feels natural and familiar to drag a Thumbnail along the ScrollBar rail/track to change the value of a property. It has visual feedback while manipulating with touch, and is intuitive to use. While I knew a ScrollBar only changes the value of a single property, it was the Thumbnail that caught my interest. If a Thumbnail were free to slide in multiple directions, I wondered if it could be used to allow the selection of one of several values. Perhaps allowing movement in both the x and y direction, contained by some bounding field, would be a potential solution. At the same time, I was thinking of how to visualise multiple dimensions in general. Because I’m a huge fan of Geometry, I typically think of geometrical shapes at times like these. This time was no exception. Blending this line of thinking with the problem of choosing from three values led me to think of a 3-dimensional geometrical shapes; Triangles. If a selectable option were placed at each corner of a Triangle, could a player drag a Thumbnail onto each option to select it? I began sketching as I was thinking: As I worked a memory was triggered; I remembered an old joystick Commodore 64 joystick with a triangular-prism for a shaft (like a Toblerone Chocolate Bar packet). You would push the shaft to indicate your choice of eight directions (nine of you count the default no-direction state). As with all digital joysticks, if you stopped pushing the shaft it would snap back to the centre. What would it look like if I merged all these ideas? A triangular Thumbnail, pushed around by touch, restricted by a triangular field, with the three selectable options at each corner, that snapped back to the centre when the gesture was complete? I began sketching what a Pause Menu with such an interface would look like: I knew from Geometry that by having the Thumbnail be a smaller-scale version of the same shape as the containing field, the Thumbnail would fit perfectly in any corner. I realised this state could be how the player could position the Thumbnail to indicate they wanted to select the option at that corner of the field. I also realised this would work for _any_ convex geometrical shape, allowing me to generate geometrical fields and Thumbnails for however many numbers of options I needed as my game development progressed. I had prior knowledge that Godot (the game engine I am using in my game) has a move-and-slide behaviour built in. It should be fairly easy to create a mechanism where a gesture-driven Thumbnail would be restricted by the edges of the field and slide snugly into a corner when pushed around by the player. It was all sounding quite promising. Checking Against the BriefMy excitement was growing at this point as I was thinking the design was going to work. As I imagined using the mechanism it satisfied the feel I was after much better than the original design. It was time to cross-check with the Brief to ensure I wasn’t missing anything important: ✅ The menu could be accessed by the same mechanism before, which was already intuitive and quick.✅ The menu had 3 options that could be chosen.✅ Because I used geometrical shapes I could flexibly generate the interface.✅ It encapsulated the feel I was after. There were a few questions remaining to be answered. Would it be easy to use? Would it be efficient to use? Would it be intuitive? Final Tweaks to the DesignAs for the intuitiveness, I needed to build a prototype and test it out for myself (and playtest it with other people as well). That wasn’t going to happen before I landed so I put that aside as future work. I had a concern about how easy and efficient it would be to use. On a phone the Thumbnail may be too small to touch with a finger, or be obscured by the finger as the player dragged it around. This was a relatively simple one to solve as I had already built a similar mechanism in my Touch Manipulation Test. Because the only touch-controllable part of the new Pause Menu is the Thumbnail, I would make it such that you didn’t need to start the gesture by being in contact with the Thumbnail itself. Wherever your finger was on the screen when you started the gesture would be the point where the Thumbnail’s position change would be calculated from as you dragged. You could push and pull the Thumbnail from anywhere on the screen you felt comfortable. This technique isn’t new, it is a well established technique in many touch interfaces. Applied here it would certainly make the Thumbnail easy and efficient to move around. With your finger out of the way the Thumbnail will be clearly visible, even on small mobile devices. The player should intuitively understand the physics when they see the Thumbnail colliding with the containing field as they move and slide the Thumbnail around. Finally, I thought by orienting the Triangle correctly, the user could intuitively expect the correct option/outcome when they swiped. Placing the “Go back to the main menu” option on the left and “Continue playing” option on the right mirrors the familiar swipe left to “Go back” and swipe right to “Go forward” on a browser. The Thumbnail would collide with the edge of the field and slide into the appropriate corner as a result of the swipe. That left “Quit the app” as either swipe up or swipe down, depending which way the triangle points. On Windows (a target platform for my game), a swipe down is used to close the app in WinRT, so I chose my triangular field to point down. Again, this keeps some familiarity among gestures. Design complete! In case you missed it, here’s a video from my last post showing the new Pause Menu in action: The redesigned Pause Menu in action SummaryCongratulations on getting to the end of this mammoth post! I hope it wasn’t too dry as I recounted my process. Communicating all that thought and reasoning in text has been challenging to make this an easy read. Remember, just like coding, design is all about: thinking at a high level to ensure you’re moving in the right direction… thinking in intricate detail to ensure everything is appropriately covered… doing both at the same time to ensure everything works harmoniously to achieve the right result. Here’s the full page scan of the page of sketches I created while I was on my flight for your reference. You may even spot some bonus sketches if you look closely! Final WordsI hope the information in this post assists you with your own design process and helps to make better software. The world has too many examples of poor design choices for my liking, and I believe as more people showcase their design processes, in more electronic mediums than just the Web, good design will become more ubiquitous and the world will be a better place. In the next post of this series I’ll get back to the programming side, and talk about how I took this design into a prototyping phase. I’ll talk about more of the challenges I faced and how I overcame them. I look forward to seeing you in the next post! PREVIOUS POSTS IN THIS SERIES Building a Pause Menu - Part 1 Building a Pause Menu - Part 2 - - The first version The rewrite showcase NEXT POST IN THIS SERIES Building a Pause Menu - Part 4 - Prototyping with move-and-slide","link":"/2020/03/15/building-a-pause-menu-part-3/"},{"title":"Red Peregrine 2.0","text":"A lot more than a facelift. Peregrine Falcon by Wallpaper Flare Skip Preface and Jump to the Post PREFACE Life has continued to throw challenges my way as we go deep into 2021. While my partner’s chronic health and pain challenges are finally moving in the right direction, it’s clear we’re a long way from getting back to normal. Recovery from surgeries is very hard, and the realisation that the hardship is far from over has caused mental health issues to surface. There’s just too many reminders of what a knife-edge her health is on; whether it’s the latest strains from hydrotherapy, the health-care subsidies running out, being unable to return to work after 7 months, or just dealing with twinges because she moved subtly wrong, being constantly reminded that this ride isn’t over is a real strain on her mental wellbeing. We’re addressing it in all the right ways, but it’s clear we’re going to be dealing with the fallout of this for longer than we want to be. The prevalence of COVID-19 still impacts our day-to-day. While we’re still very fortunate in Western Australia with very few outbreaks, the flash-lockdowns have been untimely to say the least. On one hand we’re lucky they’ve happened right about the times of school holidays. On the other, we’ve now had 2 vacations ruined by the necessity of being forced to stay at home. Chris and I really need a decent break to relax and recharge but it keeps getting torn away from us at the last minute. I support the decisions our local government makes to protect us (it clearly works) but I wish we could fast-forward our vaccination program so that the need for lockdowns would go away. As usual I’ve been throwing myself into work, because getting things done makes me feel good. We’re doing some massive and exciting things lately (which I can’t reveal too much about until we go public) and we’ve been consciously crunching to get it ready for consumption. As the lead technical engineer on the product I’ve been working very hard and longer hours than normal to ensure we keep on the right track as we push through a huge growth in the codebase. I did this consciously knowing I was going to have 10 days off with a lovely vacation swimming with Manta Rays, and it hurt deeply having an untimely lockdown take that away from me. The time off was awkward with the restrictions in place, but at least it forced me into some much-needed self-care. Wild weather has also caused unwanted disruption to our plans. We’ve had some massive rainfall over here; at one point we had 40mm rain in a single hour. During one of these storms we were close to having floodwater come into our house as the soakwells in our backyard blocked and overflowed. I spent several hours out in the freezing cold water shovelling and trying keep the water out. Meanwhile, on the other side of the house, our sagging roof caused the gutters to fail and the retaining wall holding up our house started to wash away. Fortunately it held, and I’ve been able to create a system to catch the overflow in tanks and pipe the water away where it can’t do any more damage. We’ve been trying to find someone to come and repair or rebuild our roof but aren’t having any luck. It will be months before any work can start anyway (we need to wait for dryer seasons) so hopefully we can get it sorted before more damage occurs. In the meantime, it’s been too wet and windy to do any work on my daughter’s tiny house, which has remained virtually untouched for months while we deal with all these other priorities. As for me, well I’m doing OK considering. I’m still dealing with more than my fair share of trauma in my life, but despite still struggling to look after myself first I’m holding up. Having a great therapist in my corner has really helped me to cope with everything, and I can’t wait for life to settle down a bit so I can get on the front-foot with my own mental health issues. Knowing what the issues are is more than half the battle and I know there are better times ahead. As a survivor I recognise this situation as familiar ground; even though I’m slogging it out in the mire right now I’m got my armour of resilience on and I’m still moving in the right direction. IntroductionWith all the drama of my life going on it’s been difficult to find the energy to work on my own projects. But as the multitude of challenges on my plate slips in and out of control I’ve recovered to the point I’m ready to start working on them again. I haven’t quite had the reserves to push myself into deep analysis or areas where large effort is required, so easing back into it with something familiar felt like a wise first step. When I discovered the technology I base this blog site on had moved forward 4 major versions from what I was using I began evaluating whether upgrading my site could be achieved with my tenuous energy levels. Icarus is the theme of Hexo I use to create this statically-generated website. When I picked it up for my first post back in December 2017, Icarus was only on version 0.4 and yet still stood out to me as the most functionally capable choice of all the Hexo themes. It’s now 4 years on and Icarus continues to be a popular choice for people to create their own sites with. With so many people consuming Icarus the developers have put in a lot of work. Not only does the 4.2 theme look great, it also make the features of Icarus (and Hexo) easier to use. As I browsed through Icarus’ showcase sites I realised what a breath of fresh air these improvements do to how an Icarus site feels. Comparing these showcase sites with my own site I could see how dated mine looked and I felt a little ashamed that someone with my talents ran a website that looked like it was written in the 1990’s. So it was I decided to bite the bullet and commit to upgrading my site to Icarus 4.2. That’s 4 major versions of jump right there. As such I’m sure you can appreciate what a journey it’s been. Strong FoundationsIt’s been quite a few years since I’ve done any major web work. Web development is a field I’ve only spent a few years of my career in so I wouldn’t say I’m totally comfortable working with web technology. However the Hexo system is cleverly designed and makes things so simple; I can’t help but admire the architectural decisions the developers have made to make it so easy to consume and extend. The prime example of simplicity is how posts are created using Markdown. This lovely markup language has become ubiquitous through engineering communities in the last decade or so. No matter your platform or tech-stack it’s virtually impossible to escape; it’s almost guaranteed some part of your engineering process uses Markdown. As a medium for writing documentation it is powerful and elegant, allowing content and structure to flow from thought to text with minimal friction. There are so many places Markdown is present in my daily technical operations that writing with it has become second nature. Hexo also seamlessly integrates other markup types in your posts. For instance HTML markup can be used right alongside the Markdown. This allows the creation of structured content that Markdown alone does not provide. When you generate your site with Hexo it detects the change in markup language, interpreting the Markdown parts as Markdown and the HTML parts as HTML, leveraging the power of both to output the appropriate rendering. This frictionless extensibility makes Hexo powerful while remaining friendly to use. Another markup type Hexo allows is Nunjucks. Nunjucks templates allow you to create extension points in your posts where javascript functions are executed at site generation time. Input is passed to the javascript function via arguments and content enclosed by the Nunjucks template, and the output of the function replaces the extension point described by the Nunjucks template. This extensibility means you can create reusable, complex components (such as that bodies of HTML content) accessed using succinct Nunjucks markup. Deeply complex scenarios are made possible with minimal distraction to your writing flow; complexity is abstracted away from your post while remaining easily accessible and feeling entirely natural. ASIDE Hexo extensibility goes a great deal further than this, providing a comprehensive programming model allowing multiple layers of extensibility during site generation. Custom plugins, filters, and generators can be dropped right into your website project via npm, copy/pasted from the web, or developed locally. These extensions seamlessly integrate with your Hexo web projects in a way you expect from web technology. This has been a catalyst for a vibrant and engaged community of Hexo users, willingly share their extensions and helping each other build great websites. While it’s not the topic of this post I recommend you check out the Hexo API if you have interest in the extensibility afforded by the Hexo architecture. Why are you telling me all this?The fact that posts in a Hexo site are simply Markdown script with other markup mixed creates a loose-coupling between the site content and generation/delivery technology. This means makes the task of migrating or replacing the generation engine of your site to a different technology relatively straightforward. The confidence of knowing there would be minimal breaking changes to content of my blog site if/when I needed to migrate is one of the primary reasons I found Hexo so attractive in the first place. As I began the task of upgrading through 4 major versions of the Icarus theme I was about to reap the rewards of this flexibility. Things had changed so much with Icarus I may as well have been changing my site generation engine altogether. Even though my web skills were a bit rusty, and even though Icarus had undergone many breaking changes, I could proceed with the upgrade with confidence. Further, if anything went drastically wrong during the upgrade process, I could always roll-back my changes using git and recover simply, safely, and completely. The only risk in proceeding with the upgrade is whether or not I could complete such a large change with so many unknown challenges that would need to be resolved. Peeking under the hoodLooking at Icarus 4.2 I could see its inner workings had totally changed since 0.4. First off, it had moved from using straight Stylus to the Bulma framework, leveraging this even higher level CSS framework. This has resulted in a much cleaner implementation of Icarus than I had seen before, allowing the theme to expose features in a far simpler manner. It did mean, however, that all the customisations I had made to Icarus 0.4 would no longer apply against Icarus 4.2 and would need to be recreated. Determining what had changed wouldn’t be as hard as it sounds. When I built the first version of my site I had forked the Icarus project and configured it as a resident git submodule in my web project. All the customisations I had made to the Icarus submodule were in my own git branch on that fork. All I needed to do was to pull the latest Icarus commits into my fork, examine my branch for the customisations I had made, and recreate the customisation on a new branch off the latest Icarus commit. As I recreated these customisations I would leverage the power of Bulma and the newer features of Icarus to my advantage. Starting the upgradeFirst thing I did was to go back to an empty Icarus 4.2 site with default _config files, no posts, and the minimal set of npm modules. Basically I ensured I could generate and serve the minimum version of my site locally without errors. My plan was to slowly add my posts back in, fixing and recreating my customisations as necessary. I kept a copy of my _posts folder on my desktop for easy re-introduction into this clean site using copy/paste. It felt good to be back at the minimum set of npm packages and to update them to their latest versions. I then began hand copying back my site’s configuration settings (_config file content). While a lot of the properties were obviously the same and easy to transfer (title, description, author), there were a lot of new properties that I needed to identify what they were before tinkered with them. In this early phase of the upgrade I simply marked these new properties with a big TODO comment and would circle back to these properties once I had a sufficient amount of my content restored. The configuration of all the npm packages I had removed in the cleanup I copied back in and commented them out to remind me of what needed to be done before deploying the new site. Site Header and FooterLooking at the new Icarus the first thing I noticed was that the site name in the site header had change from a “logo with text” to a “single image” arrangement. This affords strong control over how the site name looks, helping set the tone of the site. After a period of musing I realised I wanted a hand-written style on the site name. After many hours struggling to use my own hand writing I settled on using Ink Free Font instead. I converted the text to curves and adjusted things until I was happy. I kept my diving Peregrine Falcon logo, though I did rotate it to steepen the dive angle. The Icarus footer has changed a lot. Icarus takes the Creative Commons 4.0 license compliance set by Hexo seriously and shows links to the CC4.0 license, the license attribution statement (what it means to legally comply), and a link to the Icarus source Git repo. Because I was modifying the Icarus theme for my own purposes I modified the footer to remain in compliance. NOTE Icarus also changed the footer to each post, further highlighting the Creative Commons 4.0 license arrangement. I think it gracefully bookends each post and looks beautiful. Check it out when you get to the end of this post. Site fontOne of the reasons why my original site looked so dated was because I used the default site font which had no character or life. I took a long time browsing through Google Fonts, searching for a something which looked good and conveyed the tone I wanted for my site. I found quite a few fonts that seemed up to the task but fell short on implementation. I found only one suitable font with the full set of weights I needed; Red Hat Display is a beautiful font that doesn’t strain the eyes when you read while having a friendly yet professional feel. It’s amazing how a good font can transform the feeling of your site and I am extremely happy with my choice. ColoursI still really like the muted blue colour I chose for the background of my original site, and I kept it as I updated to Icarus 4.2. Icarus uses a nice blue as a primary accent colour which I continued to use unaltered in my new site. Bulma introduces some lovely state colours (aqua info, yellow warning, red critical, etc) which can be applied to HTML components to change their flavour, though I did end up replacing these styles with modified versions of the colours from my original site. The “more” button needed a lot of colour tweaking to fit with my site aesthetic, but was a simple styling exercise. Inline code blocks looked plain wrong out of the box and were suitably tweaked. Applying these types of colour changes were a simple matter of finding the CSS classes used by the newer Icarus and tweaking the values. More serious colour changes would come later on when I developed my customisations, but these simple changes would serve as a base for the time being. Meta dataIcarus 4.2 also provides a number of additional page meta data options over 0.4. I settled for adding Google’s structured metadata, which should help the Google indexer process my site once I fix my broken SEO. ASIDE In the process of writing this post I noticed that Google wasn’t indexing my site. I’m disappointed at how difficult it is to register for Google indexing and stay there. I’m sure I was being indexed when I first started blogging, so either I missed the notifications that my indexing was being dropped or it happened without any warning from Google. Once I get my new site deployed my first priority will be to figure out what I need to do to get back into the Google index. WidgetsWidget is the term used by Hexo for the block sections displayed on the left and/or right of the site’s main content column. I use most of the widgets that came with Icarus 0.4 and Icarus 4.2 has added a few more: A Donation widget lets you add buttons directing your audience to services that allow them to help pay for content. A Share widget lets you add buttons directing your audience to share the post on social media. A Google Feedburner widget allows your audience to subscribe to your feed and be notified for updates. A Google Adsense widget allows you to host ads on your site. All of these widgets are turned on by default. With the exception of the Feedburner widget, I didn’t desire any of them for my updated site so I turned them off. Unfortunately for me and other Icarus users, Google’s support for Feedburner was expiring. Attempts to ask the Icarus maintainers fell on deaf ears so I regretfully turned widget off too; I didn’t want to add a new feature that I wouldn’t be able to support within a manner of months. ProfileWith my widget selection configured I moved my attention to the configuring the Widgets I had kept. I really like the updated style of Widgets; the addition of rounded corners, the default capitalisation of headers, and the use of Bulma tags in the Tags widget. However the Profile widget needed serious attention because the spacing and size of elements was really poor and the “follow” and social media buttons were tiny once I got all my configuration in there. It wasn’t too much work to get a result I was happy with, though I am still surprised how the default look of this particular Widget is so unbalanced when compared to the other Widgets in the Icarus box. Table of ContentsDespite it being one of the most desired features I wanted for posts in the first version of my site, I never figured out how add a table of contents to each post. The Hexo documentation is unclear on how to use/apply them but the improved clarity of the Icarus 4.2 config file put me on the right path this time around. One simply adds toc: true to the front matter of each post to turn this widget on for that post. I have now added toc: true to the scaffold used to generate new posts, and would add it to my old posts manually as I restored them. The formatting Icarus 4.2 applies to the table of contents is lovely but I still needed to apply a few tweaks: I wasn’t happy with the caption of “Catalogue” so I changed it to “Index” in the English resource file. Some of my posts were so long the table of contents would run off the bottom of the screen. Given this widget scrolls with the page content this created a less than ideal reading experience. I used CSS to add a maximum height and overflow to the TOC HTML element, forcing a vertical scrollbar to appear when it was too long to fit comfortably on the browser window. Restoring the PostsNow the framework of my updated site was in place it was time to start bringing back my post content one by one and fixing anything that was broken. Because my first ever blog post contains a lot of mixed content types I felt it was the ideal candidate to restore first. Copying that first post back into my website project and generating the site revealed compile errors where Tag Helpers were missing. This was expected; having stripped back to the minimal set of npm modules I knew there were things previously provided by those npm modules that were no longer installed. The errors highlighted which Tag Helpers I needed in order to get that first post working again. Because this was my second build of my site (my first rebuild if you prefer), I was more familiar with how Hexo works. Where previously I was desperate to get my site up so I could start blogging as soon as possible, this time around I decided to focus more on polishing the look of the site. This decision gave me the time and space to to flex my engineering muscle and create my own Tag Helpers, rather than take dependencies on external ones from npm. This approach adds the benefit of making the site content more flexible should I need to do other site upgrades in the future. After building custom Tag Helpers so that first post would generate, ensuring all the content of the post was showing, and tweaking my Tag Helpers so that everything looked good, I had successfully restored my first page to the new site. This proved my restoration approach worked and I fell into a pattern of restoring my posts one at a time from oldest to newest, creating new Tag Helpers as I required and extending existing custom ones to fit changing use cases. After many iterations of restoring, coding, updating, and verifying each post I had finished re-importing all my content. It was a lot more work than I had anticipated (and subsequently took a lot longer than expected), so I felt it warranted a post of its own (this one!) documenting the experience and sharing what I’ve learned. Custom Tag HelpersSpeaking of sharing, let’s talk about all those custom Tag Helpers I made along the way. While it would have been easier to restore all the additional npm modules I had added on my original site, I’m very glad I decided to take on building these extensions as Tag Helpers myself. I was never comfortable with the lack of flexibility due to my dependence on them, and I didn’t want the overhead of going through the contribution process for the many small changes I wanted on so many small components. Sometimes I wanted dynamic control over leading or trailing space, sometimes I wanted different colours, sometimes I wanted more control over the functionality they provide. I am much happier having complete control over these components within my own repo, and enjoy not having so many npm modules cluttering up my site project. As mentioned earlier, I created and extended these Tag Helpers as I restored each post. The following sections show the final version of each Tag Helper once I had restored all my posts and written this one. Tag Helpers are listed in the order of importance to my site from most important to least. zodnoteI use note blocks proliferously in my posts. I use them to give the reader a place to rest after reading paragraphs of intense content, or for information that’s not entirely on point to the thread of the post at that moment. Previously I used the hexo-tag-admonition plugin to create these blocks, but now I wanted to have more control over the structure by leveraging Bulma and Icarus, and over colour variants using CSS. First I created the appearance I wanted directly in a post using HTML. I combined the Bulma Message component with a Bulma Tag element to form a note block with a header tag. I then added the message-immersive class I had seen used in Icarus documentation, which pushes the note to the full width of the column. With the HTML structure working I then created a zodnote Tag Helper script to output that structure. I added a parameter for base colour selection and allow the remainder of the parameter section to be the text of the Tag Header. For the base colour parameter I allow key words that map into Bulma colour modifier classes which I apply to the Message HTML element (&lt;article&gt;). I style these colour modifier classes to match my personal preferences in the web site’s article.styl Stylus file. zodnote usage (taken from my private Style Guide page) Here’s the code, including extracts from the style files to get the colours how I like them: zodtagI created the zodtag Tag Helper after seeing how the Icarus documentation gives credit to photographers when using their images. My previous approach was ugly in comparison so I blatantly stole the idea and formatting from Icarus and rolled it into a Tag Helper. photo credit: old vs new In recent times I’ve taken to adding Preface sections to my posts as a space to share personal messages from my life at the time of writing. These sections can be quite long, aren’t important to some readers, and can otherwise get in the way. I added a skip variation to zodTag to gracefully allow readers to skip over the Preface and jump straight to the content of the post. The skip variation of a zodtag Finally, I needed better control over the space between zodTags and the HTML bordering it. I added a few parameters to modify the top and bottom margins. These parameters allow either a Bulma spacing helper of mt or mb, or normal style value such as 1.7em. zodimageYou’d think embedding images were a long-solved problem in Hexo and you’d be right. Before upgrading to Icarus 4.2 I used Markdown syntax to create the images and image links in my posts without an issue. However after upgrading Icarus I noticed a few problems; image captions no longer supported embedded Markdown and image links did not work at all. My response was to create a Tag Helper that encompassed all the image functionality I need. For basic image use I need only provide the image URL to my zodimage Tag Helper. This could be a filename local to the post, a relative path reference within the site, or a full URL for references to other sites. The image comes with a default maximum height to accommodate large images without disrupting the flow of the post. For image links I allow the image URL and a navigation URL to be provided. When the reader taps the image they are navigated to this second URL. I mostly use this arrangement for providing links to my GitHub Gists when sharing code there, and have a standard image and caption for this explicit purpose: NOTE Don’t bother tapping this one; it doesn’t go anywhere as it’s only for show. Speaking of captions, zodimage also support captions with Markdown. I typically use this when referencing code symbols in the caption. On rare occasion I also use hyperlinks within the caption, though for technical reasons I have to use HTML &lt;a&gt; tags in order to achieve that: The skip variation of a zodtag NOTE Tapping this image doesn’t navigate either; it’s another sample only for show. The caption link does navigate however. Finally, zodimage allows control over image height, spacing between surrounding HTML, and spacing between the image and the caption. All of these are optional but are sometimes required to create a more appropriate layout than the default values provide. Here’s the code: zodcaptionFor those times where I want to add a caption to miscellaneous content (such as some custom HTML or a code block) I created a stand-alone caption Tag Helper. As you’d expect I style it like the caption content from all my other Tag Helpers and support Markdown and HTML links. I allow optional modification of the space above and below the caption to suit all manner of scenarios. ASIDE Given I use captions in almost all my Tag Helpers I thought about centralising the code for reuse but couldn’t figure out how. Given how entwined caption code is within those other Tag Helpers it probably isn’t a good idea anyway so I let it be. zodvideoPrior to upgrading Icarus I was using the npm library hexo-tag-owl to embed links to YouTube and Vimeo videos. Now Icarus provides that support out of the box. I usually (always?) add a caption to my embedded video, so rather than using two Nunjucks tags to embed a video and caption separately I rolled them into one Tag Helper for convenience. Simply specify the video platform, the Id of the video, and a caption, and I’m free to quickly keep writing my post: zodcolumnsThe final Tag Helper I created is a bit of a doozy; it produces the most complicated HTML of all my custom Tag Helpers. The zodcolumns Tag Helper allows me to create tables of content occupying the width of the post column. The number of rows and columns is inferred by the content, and I allow an optional background colour. Title rows can also be embedded: ARGUMENTS colour type url mt:x mb:y - - - - - black | green photo | skip an optional link optional top margin where x is a size or an mt Bulma spacing helper (mt-[1,2,3,4,5,6]) optional bottom margin where y is a size or an mb Bulma spacing helper (mb-[1,2,3,4,5,6]) zodcolumns used to describe how to use the zodtag Tag Helper on my private Style Guide page Basically zodcolumns is a wrapper for Bulma Columns elements, which is an arrangement of class-marked divs styled into columns and rows using CSS. I use the is-narrow columns variation which aligns all the columns I specify to the left hand side. I allow special delimiters in the content of zodcolumns to indicate the start of a new column (/) or a title row (/title). Line feeds determine the number of rows in the column. Title rows aren’t actually part of the table, they’re just a &lt;p&gt; block above the table which I’ve styled to look the same as headers in Icarus widgets. 12345678910111213141516171819202122{% zodcolumns %}/titlearguments/`colour``type``url``mt:x``mb:y`/-----/`black` | `green``photo` | `skip`an optional linkoptional top margin where x is a size or an `mt` [Bulma spacing helper](https://bulma.io/documentation/helpers/spacing-helpers/) (`mt-[1,2,3,4,5,6]`)optional bottom margin where y is a size or an `mb` [Bulma spacing helper](https://bulma.io/documentation/helpers/spacing-helpers/) (`mb-[1,2,3,4,5,6]`){% endzodcolumns %} The zodcolumns definition for the above example. Pretty simple right? The real reason I made this Tag Helper is for displaying links to other posts in a series of posts. For these I colour the table using the optional colour parameter, and use multiple title rows to carve up the table into relevant sections. The result is a professional looking block in my posts that feels right at home among the other components and widgets provided by Hexo and Icarus: PREVIOUS POSTS IN THIS SERIES Building a Pause Menu - Part 1 Building a Pause Menu - Part 2 - - The first version The rewrite showcase NEXT POST IN THIS SERIES Building a Pause Menu - Part 4 - Prototyping with move-and-slide Swish! The zodcolumns table used in part 3 of my series on Building a Pause Menu in Godot Compare this to how I used to display these links; there’s no question that using zodcolumns is more visually appealing: An image of the old way I displayed the same table. The 1990’s called and want their style back. 😁 You might think the code producing all this would be complicated but it’s actually very simple. Most of the functionality is provided by Bulma. I simply deparse the content and convert it into &lt;divs&gt; with appropriate class properties. The magic of CSS does all the rest: More about IcarusApart from new widgets and components, Icarus 4.2 provides a lot of other plugins out of the box. Let’s talk about these other features I use. MathJaxWhile I originally installed the hexo-filter-mathjax npm module for my math-heavy article on k-combinations, Icarus 4.2 comes preloaded with MathJax pre-installed. Rather than have two MathJax libraries in my site I decided to try rolling with the version bundled with Icarus to see if it would support what I needed. I did notice some minor differences right away when converting my k-combinations post. I had to modify my textcolor tags down to color tags to get the inline colours working again. The colours on the Icarus plugin are more muted than with hexo-filter-mathjax so I switched colours altogether. Icarus MathJax is a little more compact than before, and feels very much at home with my new font choice. I find I prefer the Icarus version of MathJax: MathJax content: old vs new There is a drawback to relying on Icarus for MathJax; you have to turn on MathJax support at the site level rather than the post level. That means every post on the site loads an additional 128kb of script MathJax script, whether you use it in the post or not. 🤨 Worse, the MathJax script can match content as MathJax even when you don’t want it to. This means I had to recheck the content in all my posts to ensure MathJax didn’t mess anything up. Fortunately I found only the GitHub Gists in this post were affected. Thanks to help from the folks who maintain Icarus I was able to wrap the affected content in a div tag and instruct MathJax not to process it. 123&lt;div class=\"tex2jax_ignore asciimath2jax_ignore\"&gt;&lt;script src=\"https://gist.github.com/ZodmanPerth/ce90e1ac74276f5e30f0905c71a91c5d.js?file=Sample.js\"&gt;&lt;/script&gt;&lt;/div&gt; No MathJax is applied on this Gist! 😊 Personally I would have preferred if MathJax was opt-in on each post it was required for by using a directive in the front matter. I may also have liked to provide special &lt;div&gt; content to instruct the MathJax script which content I wanted processed, rather than the opt-out option provided by MathJax. Once I deploy the new site I will be watching the impact on my Google rankings closely, once I get back in the index of course. 😞 Column Widths and Media QueriesIcarus implements a 12-column grid layout, dividing the usable space on the web page into 12 available columns. It then provides configurations for both 2 and 3 column posts where the widgets can be located on one or both sides of the post column. This is a widely adopted standard and allows software engineers to create adaptive content layouts with minimal complexity. However when it comes to changing the size of those columns in Icarus things get a little tricky. My site uses a 3-column layout (widgets on both sides) and my posts are a wordy enough to look uncomfortable in the space afforded by the default layout on wide screens. It took me a long time to figure out how to make the center column (the post content column) wide enough to feel comfortable on widescreen displays, stay centered on the screen horizontally, and still allow the page to responsively rearrange itself on smaller screens (like tablets and laptops). At first I thought changing the CSS column class generated into the post was what I needed to change. I fiddled with these classes in layout.jsx and widgets.jsx but things seemed to skew towards one side of the page instead of staying centered. It was then that I realised Icarus 4.2 is missing the 3-column layout code media queries for larger displays: Git-diff of the missing 3-column states This helped align things centrally again, but the post column was too small. I needed to tweak the default media query breakpoints set by Icarus to get things to work like I wanted. The default values look strange to me, and the size I set for fullhd is how I tweaked the size of my post column to look how I wanted on a widescreen monitor: 12345$gap ?= 64px$tablet ?= 768px // 769px$desktop ?= 1024px // 1088px$widescreen ?= 1440px // 1280px$fullhd ?= 1860px // 1472px Original/default breakpoints are shown as comments Style GuideWith all this customisation going on it was becoming difficult to maintain documentation on how everything worked. Where previously I was using a private readme.md to keep a reference for the syntax for all the different Nunjucks and HTML hacks I use to create posts, I was struggling to use Markdown alone to document and demonstrate the syntax correctly. I decided to create a Style Guide page as a draft post to document everything effectively. As a draft post I can generate and serve the Style Guide page for reference while I’m creating content and generate the site without it when deploying the site for public consumption. The main benefit of using an actual page for the Style Guide is that I can demonstrate how things look visually when they are used in a post. This really helps organise the information effectively, as well as making things much easier to find using either the scrollbar or the Table of Content. Unfortunately, as a draft post, I can’t give you a link to the page to peruse. Sure I could deploy the page as a normal post but as it has nothing to do with my blog content-wise I don’t want that. You’ll just have to drool over this video of me scrolling through the page to get a feel of how useful it is to me: A video walkthrough of the Style Guide page ConclusionHexo is delightfully extensible and Icarus is a solid foundation for creating web sites. Upgrading Icarus through 4 major versions to 4.2 was relatively simple. I’ve shared a lot of the major changes I’ve done on top of Icarus such as Tag Helpers and CSS. In addition I’ve made a bunch of minor tweaks which I haven’t blogged about here, but everything combines to making my site look great and I’m a lot happier with how the site looks now. Further, I have the tools to create content with even less disruption to my writing flow than ever before. I’m very glad for choosing Hexo and Icarus and look forward to creating more content with this new setup. As usual, here’s the Gist containing all the code from this post: See all the code on my Github Gist Don’t forget to check out the delicious new Icarus 4.2 post footer below. Yum!","link":"/2021/08/25/red-peregrine-2-0/"},{"title":"Software Algorithms for k-combinations","text":"Interpreting the mathematics of k-combinations for computer programmers. Crystal Pyramid Wallpaper by Michael Dziedzic Skip Preface and Jump to the Post PREFACE It’s been a little over a year since my last post which may lead one to wonder if I had dropped off the face of the earth or merely given up. Fortunately neither is even close to being true. Unless you’re a Sleeping Beauty or Rip Van Winkle, you will have noticed 2020 was the year of COVID-19. We’ve been extremely fortunate here in Australia; we locked down early enough to eradicate most of the community spread. We’ve suffered only a little over 900 deaths so far which is in stark contrast to most other nations and the total world death count of over 2,887,000 people (figures correct at time of writing). While Australians were virtually unaffected by the virus itself we were heavily affected in other ways. Firstly, lockdowns played havoc to our supply chains. Western Australia is the most isolated capital city in the world, and our supermarkets moved quickly to curb panic-buying by enforcing restrictions. It worked eventually, but not before our supermarkets were stripped bare by fear-driven mobs before the restrictions could be put in place. The lack of supply played havoc for my family, especially due to our soy- and rice-milk drinking ways (which were classified as “long-life milk” and restricted). I would spend hours travelling across multiple neighborhoods each day, going shop to shop to gather enough food and supplies for our daily meals. Fortunately we had no problems with toilet paper, which seemed to be a problem for many people. 🧻 My partner and I removed our kids from school early over health concerns, and a week later the government sent children home en masse. Online learning was organised, but overworked teachers don’t have time to keep up with technology let alone move their entire syllabus to it, so the quality was unusable. We couldn’t keep up with our kids questions about the incomprehensible instructions they were given while holding down our full time remote jobs. The constant attention required to make it work was too demanding of our time and we were forced to abandon our children’s education altogether during this remainder of lockdown. As most of Australia came out of lockdown and life found the new normal for most folks, my partner began suffering a debilitating medical issue and couldn’t get out of bed or move at all on her own. I took up the additional responsibility of full-time carer for our children and for her. Our public hospital system failed her utterly, refusing to provide a bed, adequate medical relief, or empathy from the agonising pain that wracked her body around the clock. After weeks of broken sleep, mental anguish, and helplessness at being unable to relieve her constant cries of pain, we threw a haymaker and went to a private hospital. Thankfully she immediately started getting the right treatment at last, putting her on track for multiple back surgeries to repair what turned out to be the worst bulging disc in anyone’s peacetime experience. She is still recovering but is finally doing okay. During this period of caring for her, being a sole-parent, working full-time, and hour-long round-trips to the hospital far far away, my mental health started to break. Now anyone who’s played cricket with or against me knows I am one mentally tough hombre, but after months of my unrelenting pressure my grey-matter said “no”. As luck would have it I had already organised a therapist for myself weeks earlier and right at my breaking point I had the help I needed on hand. As a bonus we determined I suffer from CPTSD caused by major childhood trauma. This was actually a huge relief to me; it explained many life-long issues and forced me to direct some care on myself for a change. Sure it meant living with the COVID-shitstorm of the present and unearthing unholy childhood trauma simultaneously, but y’all know I relish a challenge. At the end of this I’m going to be one goddamn unstoppable force of nature. My partner appears to be recovering at long last, and I look forward to the day she’s fully recovered. It’s been a long hard road for both of us. Our kids have been resilient throughout this whole reign of fire and I believe its a testament to our amazing parenting skills. Now my life is getting back on track I’ve been reclaiming some spare time and dipping my toes into side projects once again. I thought I was starting gently, but as you’ll see the adventures just keep coming for me. 😁 I look forward to my partner’s full recovery, the COVID vaccine to roll out and be a success 🤞 , and to my recovery from a lifetime of undiagnosed mental trauma. IntroductionComputer programmers, software engineers, software developers; no matter what we call ourselves, there are times where all of us find ourselves faced with some logic we know is a solved problem and reach out on the internet for the solution. Most of the time that solution can be found by spending a few minutes on our favourite search engine. We quickly gain knowledge of how the solution works, most often with code to help us understand the explanation. We take the code offered, tinker with it to make it our own, and move on having learned something. This was not the case for me recently when it came time to work with algorithms I needed for what mathematicians call k-combinations. While I quickly found information about the mathematics of k-combinations, I found precious little about converting that mathematics in to code. Having spent weeks of spare time working on a problem that I expected would only take a few minutes, and with encouragement from fellow engineers, I realised there was a hole in our online knowledge base about this important topic. This post attempts to fill that void. How it Started25 years ago I wrote an application in Microsoft Excel to help me pick numbers for the national lottery. In the mid 90’s when I wrote this app, the biggest (and only) lottery available in my hometown city of Perth (Western Australia) was the Saturday Lottery Draw. In this lottery, every game required you to pick 6 numbers from 45 numbers, and you could play as many games as you like every week for an increasing amount of cost. Like any gambler worth their salt, I have my own superstitions when it comes to the selection of my numbers. Here are mine: Make sure you play every number in the field at least once, across a minimal number of games If you don’t win any prize in the weekly draw, replay the same numbers the following week If you do win any prize in the weekly draw, regenerate a new set of numbers to play You don’t have to be a math genius to realise that to satisfy #1 you need to play a minimum of 45 ÷ 6 = 7.5 games. I chose to play 12 games every week, so I could cover the field once, take the three numbers left over from game 8 with the left over from game 8 in a second field, and fill 4 more games with randomly selected games from that second field. This was fairly simple to achieve with Excel VBA. I wrote two columns of numbers from 1 to 45 (the two fields), shuffled the columns using VBA’s random number generator, then coloured each group of 6 (each game) with different colours. I manually transfer my picks onto paper coupons and play them each week. 25 years later I’m still waiting for that big win. The gambler’s conspiracy voice in my head has been at me for decades to change my approach. Being a vastly more seasoned programmer today than I was when I wrote that app, one day I decided to indulge my inner gambler and rewrite the app using my current technology stack (C# with a UWP front end). You know, purely for scientific purposes 🤨. It wouldn’t take long, right? 🤦‍♀️ New SuperstitionsSoftware engineers are very good at identifying patterns, and I am quite exceptional at it. After decades of using my Excel App I’ve noticed what appears to be the same groups of numbers appearing over and over. The scientist in me theorises the 25+ year old VBA randomizer probably doesn’t have the best random distribution on the planet, so I wondered if the .NET randomizer would be any better. It wasn’t hard to whip up some code in LinqPad to show the distribution, and sure enough it was terrible. I toyed with the .NET Cryptogrphic Number Generator and was equally dissatisfied. I searched NuGet for a better alternative, and settled on RandN which has a specific aim to improve the quality of the randomness. Testing the distribution of the various generators RandN provides also left me dissatisfied. Here’s a sample distribution over 100 million generations of numbers between 1 and 45 using it’s StandardRng strategy: NOTE While not lighting my world on fire with it’s random distribution, I do find RandN to be a solid random number generator. I continue using it to provide random numbers for this project and recommend it over .NET’s built in randomizers. After some experimentation with generating lotto games randomly, and noticing certain combinations repeating too often for my liking, I resigned myself to the fact that simply using a random number generator was not going to be enough to satisfy my inner conspiracist. I would need to do more work to guarantee I don’t play the same numbers more than once. It should be obvious there will be a finite number of ways to choose 6 numbers from 45 numbers. I figured if I precalculate every possible combination of 6, select my games randomly from this list, and store the combinations I have played, then I can guarantee I won’t be repeating games until I have depleted the entire list. I need to do additional work to ensure I don’t choose combinations that violate superstition #1 above, but that’s the kind of algorithmic challenge that I love about writing code; I have faith in myself to resolve that. I just need to generate that list right? Another Numbers GameTo track the games I’ve played I would be storing a large list of sequences of 6 numbers on disk, so I need to consider how much space that takes. The worst case with minimal decoration will store all combinations of 45 choose 6, so how many combinations are there? Mathematics has a field of study called Combinatorics that is devoted to counting systems of finite and infinite sets of numbers. It doesn’t take much searching to discover that in the language of mathematics I’m asking the question “how many combinations are there of n choose k?”. This is a solved problem in mathematics; the number of k-combinations is the count of the finite set of unique ways to choose k combinations from n things. From there it’s easy to find the formula for the Binomial Coefficient which is used to calculate the count of n choose k. $$\\bigg(\\vcenter{n \\atop k}\\bigg)$$ How to write n choose k in mathematics I won’t go into the math just yet, but using this formula I found the number of combinations I would need to store is 8,145,060. That’s a big number! If I store all 6 numbers of each combination as a 12 digit string with no separator, this results in a file size of over 85MB! That’s an excessive amount of data for someone who grew up in the 8 bit era, and I simply couldn’t find peace until I found a better storage mechanism. Can You Smell What The Math Is Cooking?One tool computer scientists use to identify entries in a consistently repeatable sequence of data (such as my list of all possible sequences) is an index. I can just number each entry in my list of all possible games with an integer from 1 to 8,145,060 as long as I can generate the same sequence every time I need. If I store each index on it’s own line in the file, that gives me a maximum file size of just under 69MB. While that is an improvement over 85MB (almost 20% less data), there are more efficient ways to store index data than as ASCII. What’s important to realise though is that indexing is probably my best way to reference previously played combinations. When it comes to Combinatorial Number Systems, mathematics has us covered there too. In combinatorics, the index of an entry in the finite set of $(n \\space choose \\space k)$ is called the rank, and finding the index/rank of a k-combination from the set $(n \\space choose \\space k)$ is called ranking. Now mathematicians love a good bijection as much as any computer scientist (maybe even more…purist bastards! 😁), so with typical swagger mathematics defines the inverse operation as well. In combinatorics finding the k-combination at a specific index/rank from the set $(n \\space choose \\space k)$ is called unranking. At least the naming convention makes implicit sense! ASIDE In combinatorial number systems, the order in which k-combinations of the set $(n \\space choose \\space k)$ are listed is called a Lexicographical Order. This order must be idempotent so that mathematical functions like rank and unrank can be bijections of each other. We don’t go into detail about these terms are used in this post, but you will need to understand them if you choose to read wider into the mathematical theories of k-combinations. Both ranking and unranking operations will be required in my new lotto game generating app. I’ll need to generate the set of all combinations of $(45 \\space choose \\space 6)$ using a for loop over all the possible indexes 1 to 8,145,060 (unranking) , and I’ll need to regenerate the index from the combination when I’m caching the played games to disk (ranking). NOTE The functions rank and unrank are usually accompanied by an iterator function to calculate the next k-combination in a sequence of $(n \\space choose \\space k)$. This function is used to enumerate the entire set, which is more efficient and performant than unranking each combination from scratch using an incrementing index. For the values of $n$ and $k$ that I’m using the performance gain isn’t relevant so I chose to keep the code simple by not implementing it. Where’s Wally?Knowing what the mathematical terms are is one thing, but finding examples of algorithms implementing them is something else. For programmers, the most helpful Stack Overflow contributions on the topic often refer to an old MSDN article that has been retired, and currently available only by downloading a massive PDF archive of MSDN that includes it. For mathematicians, community contributions gloss over much of the reasoning that programmers need to create algorithms. I suppose in that respect at least the programming and mathematical communities have that in common. 😜 After a lot of searching I concluded that finding algorithms for rank and unrank weren’t available to read, understand, and use like most solved problems generally are. In terms of a complete reference of how to think about those mathematical functions, the best example I could find was definitely that retired MSDN article by Dr James McCaffrey, which I’ve dug out and am now hosting for posterity and your reading pleasure. Armed with Dr McCaffrey’s article in particular, and supported by a host of other mathematical and Wikipedia web pages, I began to decode, reproduce, and create the functions I needed in C#. Algorithm ArchaeologyThe article describes the algorithm for unranking a k-combination from it’s index, as well as providing some C# code to do it. While the code is welcome and helps to verify the intention of the lengthy description, the description itself is the gold I needed to find in order to resurrect the knowledge. As for the code, I most often I prefer to re-write code myself to embed the knowledge in my own mind and incorporate language features that didn’t exist when reference code was written (simplifying it for maintainability and unit testing). Further, the reference code uses a zero-based system whereas the solution I’m after requires a one-based system. After a lot of sweat and experimenting, I managed to distill the algorithm for a one-based unrank function down to the pseudocode below. The steps required in the algorithm are on the left, while some example working out using the set $(\\color{yellowgreen}5 \\space choose \\space \\color{skyblue}3)$ with an index of $\\color{yellowgreen}1$ is on the right. Ignore the unfamiliar terms as I’ll define what they are a little later on in this post. 1234567Take n and k | n = 5, k = 3, (n choose k) = 10Take the index we want | 1Move to base 0 | index - 1 = 0Calculate the dual | (n choose k) - 1 - index = 10 - 1 - 0 = 9Calculate combinadic of the dual | combinadic of dual = combinadic(9) = (4,3,2)Map to zero-based combination | (n-1) - combinadic elements = 4 - (4,3,2) = (0,1,2)Add 1 (for base 1) | (1,2,3) The pseudocode for the Unrank function Let’s step through the algorithm. Take n and kThis simply defines the values of n and k, and the value of (n choose k). Take the index we wantThis defines the index/rank of the k-combination we want to retrieve from the set $(n \\space choose \\space k)$ Move to base 0Converts the system to a zero-based system. Ignore this step if you already use base zero. Calculate the dualCalculates the dual of the index/rank. Calculate combinadic of the dualCalculates the combinadic of the dual. Map to zero-based combinationMaps the combinadic to the k-combination in a zero-based system. Add 1 (for base 1)Because I want one-based combinations, I add 1 to the result. Fairly simple right? But what are those undefined terms? What do they mean? Calculating n choose kAs mentioned previously, finding the value of $(n \\space choose \\space k)$ is a solved problem. In mathematics we use the Binomial Coefficient to calculate this value, and forms of the algorithm are explicitly described in the Combinations Wikipedia page.Here’s my version, which is based on the second alternate computation method from that page: 1234567891011121314151617181920212223242526// / \\// | n | from n// | |// | k | choose k// \\ /// Kudos: The 2nd alternate computation of https://en.wikipedia.org/wiki/Combination#Example_of_counting_combinations/// &lt;summary&gt;Calculates the Binomial Coefficient (n choose k), which is the number of unique k-sized combinations in a set of n elements.&lt;/summary&gt;/// &lt;remarks&gt;We divide before multiplying to minimise the chance of overflow.&lt;/remarks&gt;public int CalculateBinomialCoefficient(int n, int k){ if (k &gt; n) return 0; if (k == n) return 1; double result = n; // n choose 1 for (double i = 1; i &lt; k; i++) { var numerator = n - i; double factor = numerator / (i + 1); result *= factor; } return (int)result;} NOTE I use int because my values for $n$ and $k$ are small enough to stay within the limits of that C# type. You’ll likely find the maximum value of int will become too limiting very quickly and you’ll likely need types with larger maximums. The DualThe Dual (as Dr. McCaffrey calls it) is defined as the value of $(n \\space choose \\space k)$ minus the rank/index of a k-combination. For example, if we list out all the possible combinations of $(5 \\space choose \\space 3)$, the index/rank runs from $[1..10]$ and the duals run from $[10..1]$. While my lottery generator needs to work with a one-based numbers, we’ll list the zero-based equivalent of $(5 \\space choose \\space 3)$ as well. This will become important for reference later in this article. 12345678910111213141516 (5 choose 3) one-based system zero-based system+-------------------------------+ +-------------------------------+| combination index dual | | combination index dual || ----------- ----- ---- | | ----------- ----- ---- || (1,2,3) 1 10 | | (0,1,2) 0 9 || (1,2,4) 2 9 | | (0,1,3) 1 8 || (1,2,5) 3 8 | | (0,1,4) 2 7 || (1,3,4) 4 7 | | (0,2,3) 3 6 || (1,3,5) 5 6 | | (0,2,4) 4 5 || (1,4,5) 6 5 | | (0,3,4) 5 4 || (2,3,4) 7 4 | | (1,2,3) 6 3 || (2,3,5) 8 3 | | (1,2,4) 7 2 || (2,4,5) 9 2 | | (1,3,4) 8 1 || (3,4,5) 10 1 | | (2,3,4) 9 0 |+-------------------------------+ +-------------------------------+ The CombinadicIn the decimal number system we can represent a number by breaking it down into it’s unit columns. For example, the decimal number $\\color{yellowgreen}{4096}$ can be broken down as follows: $$\\color{yellowgreen}{4096} =(\\color{yellowgreen}4 * \\color{skyblue}{1000})+(\\color{yellowgreen}0 * \\color{skyblue}{100})+(\\color{yellowgreen}9 * \\color{skyblue}{10})+(\\color{yellowgreen}6 * \\color{skyblue}1)$$ These unit columns can be represented as a powers of ten with the exponent changing by one for each column; the ones column is $\\color{skyblue}{10^0}$, the tens column is $\\color{skyblue}{10^1}$, the hundreds column is $\\color{skyblue}{10^2}$, and so on. $$\\color{yellowgreen}{4096} =(\\color{yellowgreen}4 * \\color{skyblue}{10^3})+(\\color{yellowgreen}0 * \\color{skyblue}{10^2})+(\\color{yellowgreen}9 * \\color{skyblue}{10^1})+(\\color{yellowgreen}6 * \\color{skyblue}{10^0})$$ This method for representing numbers can also be used by other numerical systems. The binary number system can be represented with each unit column as a power of two. The decimal number $\\color{orange}{19}$ is $\\color{yellowgreen}{10011}$ in binary, which is broken down as: $$\\color{orange}{19} =\\color{yellowgreen}{10011} =(\\color{yellowgreen}1 * \\color{skyblue}{2^4}) +(\\color{yellowgreen}0 * \\color{skyblue}{2^3}) +(\\color{yellowgreen}0 * \\color{skyblue}{2^2}) +(\\color{yellowgreen}1 * \\color{skyblue}{2^1}) +(\\color{yellowgreen}1 * \\color{skyblue}{2^0})$$ Now lets consider a combinatorial number system. In a combinatorial number system we can represent any number by breaking it down into unit columns just like the decimal and binary systems. However instead of representing each unit column as a power of some base integer, we’ll use binomial coefficients. For a finite set $(\\color{yellowgreen}n \\space choose \\space \\color{skyblue}k)$, the left-most unit column starts with a base of $\\color{skyblue}k$, the next column has base $\\color{skyblue}{k-1}$, and so on until the final column with $\\color{skyblue}k=\\color{skyblue}1$. We can express the representation of any number $\\color{orange}z$ of $(\\color{yellowgreen}n \\space choose \\space \\color{skyblue}k)$ as: $$\\begin{eqnarray*}&amp;\\color{orange}z=(\\color{yellowgreen}{c_1} \\space choose \\space \\color{skyblue}k)+(\\color{yellowgreen}{c_2} \\space choose \\space \\color{skyblue}{k-1})+ … +(\\color{yellowgreen}{c_k} \\space choose \\space \\color{skyblue}1)\\\\&amp;\\space\\\\&amp;\\equiv\\\\&amp;\\space\\\\&amp;\\color{orange}z=\\bigg(\\vcenter{\\color{yellowgreen}{c_1} \\atop \\color{skyblue}k}\\bigg)+\\bigg(\\vcenter{\\color{yellowgreen}{c_2} \\atop \\color{skyblue}{k-1}}\\bigg)+ … +\\bigg(\\vcenter{\\color{yellowgreen}{c_k} \\atop \\color{skyblue}1}\\bigg)\\\\\\end{eqnarray*}$$ When the values of $\\color{yellowgreen}{c_i}$ are chosen such that each $\\color{yellowgreen}{c_i}$ is greater than the $\\color{yellowgreen}{c_{i+1}}$ following it (or zero if it follows a zero), then the set of all $\\color{yellowgreen}{c_i}$ is what Dr McCaffrey calls the Combinadic. As an explicit example, let’s look at the representation of the number $\\color{orange}7$ in the set of numbers $(\\color{yellowgreen}5 \\space choose \\space \\color{skyblue}3)$. $$\\begin{eqnarray*}&amp;\\color{orange}7=(\\color{yellowgreen}4 \\space choose \\space \\color{skyblue}3)+(\\color{yellowgreen}3 \\space choose \\space \\color{skyblue}2)+(\\color{yellowgreen}0 \\space choose \\space \\color{skyblue}1)\\\\&amp;\\space\\\\&amp;\\equiv\\\\&amp;\\space\\\\&amp;\\color{orange}{7}=\\bigg(\\vcenter{\\color{yellowgreen}4 \\atop \\color{skyblue}3}\\bigg)+\\bigg(\\vcenter{\\color{yellowgreen}3 \\atop \\color{skyblue}2}\\bigg)+\\bigg(\\vcenter{\\color{yellowgreen}0 \\atop \\color{skyblue}1}\\bigg)\\\\&amp;\\space\\\\&amp;\\equiv\\\\&amp;\\space\\\\&amp;\\color{orange}{7}=\\color{yellowgreen}{4} +\\color{yellowgreen}{3} +\\color{yellowgreen}{0}\\\\&amp;\\space\\\\&amp;⟹\\\\&amp;\\space\\\\&amp;combinadic(\\color{orange}{7})=(\\color{yellowgreen}{4},\\color{yellowgreen}{3},\\color{yellowgreen}{0})\\\\\\end{eqnarray*}$$ We’ll discuss how to calculate the combinadic of numbers a little later. Combining the Combinadic and the DualIt turns out the combinadic has a very useful property for working with k-combinations in zero-based combinatorial number systems $(n \\space choose \\space k)$: when each $c_i$ of the combinadic of any dual is subtracted from the number $n-1$, it reveals the zero-based k-combination of the rank/index of that dual. That’s quite a mind-bender to take in, so here’s what this property looks like for the complete set of duals in the set $(5 \\space choose \\space 3)$: 123456789101112dual \"choose\" representation combinadic (n-1) - combinadic rank---- --------------------------- ---------- --------------------- ---- 9 = 4+3+2 = (4 c 3) + (3 c 2) + (2 c 1) ⟹ (4,3,2) 4 - (4,3,2) = (0,1,2) ≡ 0 8 = 4+3+1 = (4 c 3) + (3 c 2) + (1 c 1) ⟹ (4,3,1) 4 - (4,3,1) = (0,1,3) ≡ 1 7 = 4+3+0 = (4 c 3) + (3 c 2) + (0 c 1) ⟹ (4,3,0) 4 - (4,3,0) = (0,1,4) ≡ 2 6 = 4+1+1 = (4 c 3) + (2 c 2) + (1 c 1) ⟹ (4,2,1) 4 - (4,2,1) = (0,2,3) ≡ 3 5 = 4+1+0 = (4 c 3) + (2 c 2) + (0 c 1) ⟹ (4,2,0) 4 - (4,2,0) = (0,2,4) ≡ 4 4 = 4+0+0 = (4 c 3) + (1 c 2) + (0 c 1) ⟹ (4,1,0) 4 - (4,1,0) = (0,3,4) ≡ 5 3 = 1+1+1 = (3 c 3) + (2 c 2) + (1 c 1) ⟹ (3,2,1) 4 - (3,2,1) = (1,2,3) ≡ 6 2 = 1+1+0 = (3 c 3) + (2 c 2) + (0 c 1) ⟹ (3,2,0) 4 - (3,2,0) = (1,2,4) ≡ 7 1 = 1+0+0 = (3 c 3) + (1 c 2) + (0 c 1) ⟹ (3,1,0) 4 - (3,1,0) = (1,3,4) ≡ 8 0 = 0+0+0 = (2 c 3) + (1 c 2) + (0 c 1) ⟹ (2,1,0) 4 - (2,1,0) = (2,3,4) ≡ 9 This means that if we have any rank/index of $(n \\space choose \\space k)$, we can use the combinadic of the dual of that number to give us the k-combination at that rank/index. This is only true for zero based combinatorial number systems, but it isn’t too hard to use a bit of addition and subtraction at key stages to get to a one-based system like I need for my lottery app. How exactly does the relationship between the combinadic, dual, and $n-1$ produce the k-combinations of $(n \\space choose \\space k)$? Dr. McCaffrey doesn’t explain in his article, and I haven’t found any explanation in any of my searches across the web. I’m sure there is mathematics somewhere to prove this relationship, but I’ve just not found any. 🤷‍♀️ Calculating the CombinadicFortunately Dr. McCaffrey does explain how to calculate the combinadic of a number, at least for zero-based systems. Remember, the definition of the combinadic of a number $\\color{orange}z$ satisfies the equation: $$\\color{orange}z=(\\color{yellowgreen}{c_1} \\space choose \\space \\color{skyblue}k)+(\\color{yellowgreen}{c_2} \\space choose \\space \\color{skyblue}{k-1})+ … +(\\color{yellowgreen}{c_k} \\space choose \\space \\color{skyblue}1)$$ We need to calculate the values of $\\color{yellowgreen}{c_i}$ such that each $\\color{yellowgreen}{c_i}$ is greater than the $\\color{yellowgreen}{c_{i+1}}$ following it (or zero if it follows a zero). Let’s work through the example of $\\color{orange}7$ from the set $(\\color{yellowgreen}5 \\space choose \\space \\color{skyblue}3)$. $$\\color{orange}7=(\\color{yellowgreen}{c_1} \\space choose \\space \\color{skyblue}3)+(\\color{yellowgreen}{c_2} \\space choose \\space \\color{skyblue}2)+(\\color{yellowgreen}{c_3} \\space choose \\space \\color{skyblue}1)$$ Finding&nbsp;$\\color{yellowgreen}{c_1}$First we look for the largest value of $(\\color{yellowgreen}{c_1} \\space choose \\space \\color{skyblue}3)$ that doesn’t exceed $\\color{orange}7$. We know the maximum number possible will be $(\\color{yellowgreen}5 \\space choose \\space \\color{skyblue}3)$, so we work our way down from $\\color{yellowgreen}5$ until we find a value $\\leq \\color{orange}7$. Now $(\\color{yellowgreen}5 \\space choose \\space \\color{skyblue}3) = \\color{yellowgreen}{10}$, but $(\\color{yellowgreen}4 \\space choose \\space \\color{skyblue}3) = \\color{yellowgreen}4$, so we’ve discovered that $\\color{yellowgreen}{c_1}=\\color{yellowgreen}4$. $$\\begin{eqnarray*}&amp;\\color{orange}7=\\color{yellowgreen}4+(\\color{yellowgreen}{c_2} \\space choose \\space \\color{skyblue}2)+(\\color{yellowgreen}{c_3} \\space choose \\space \\color{skyblue}1)\\\\&amp;\\space\\\\&amp;\\equiv\\\\&amp;\\space\\\\&amp;\\color{orange}3=(\\color{yellowgreen}{c_2} \\space choose \\space \\color{skyblue}2)+(\\color{yellowgreen}{c_3} \\space choose \\space \\color{skyblue}1)\\end{eqnarray*}$$ Finding&nbsp;$\\color{yellowgreen}{c_2}$Next we look for the largest value of $(\\color{yellowgreen}{c_2} \\space choose \\space \\color{skyblue}2)$ that doesn’t exceed $\\color{orange}3$. We know the maximum number possible will be $(\\color{yellowgreen}5 \\space choose \\space \\color{skyblue}2)$, so we work our way down from $\\color{yellowgreen}5$ until we find a value $\\leq \\color{orange}3$. Now $(\\color{yellowgreen}5 \\space choose \\space \\color{skyblue}2) = \\color{yellowgreen}{10}$, $(\\color{yellowgreen}4 \\space choose \\space \\color{skyblue}3) = \\color{yellowgreen}6$, but $(\\color{yellowgreen}3 \\space choose \\space \\color{skyblue}2) = \\color{yellowgreen}3$, so we’ve discovered that $\\color{yellowgreen}{c_2}=\\color{yellowgreen}3$. $$\\begin{eqnarray*}&amp;\\color{orange}3=\\color{yellowgreen}3+(\\color{yellowgreen}{c_3} \\space choose \\space \\color{skyblue}1)\\\\&amp;\\space\\\\&amp;\\equiv\\\\&amp;\\space\\\\&amp;\\color{orange}0=(\\color{yellowgreen}{c_3} \\space choose \\space \\color{skyblue}1)\\end{eqnarray*}$$ Finding&nbsp;$\\color{yellowgreen}{c_3}$There can be only one value for any remaining $\\color{yellowgreen}{c_i}$ once we get to $\\color{orange}0$, and that’s $\\color{yellowgreen}0$. So we’ve discovered that $\\color{yellowgreen}{c_3}=\\color{yellowgreen}0$. The ResultNow we have all the values of $\\color{yellowgreen}{c_i}$ the combinadic is revealed: $$\\begin{eqnarray*}&amp;\\color{orange}7=(\\color{yellowgreen}4 \\space choose \\space \\color{skyblue}3)+(\\color{yellowgreen}3 \\space choose \\space \\color{skyblue}2)+(\\color{yellowgreen}0 \\space choose \\space \\color{skyblue}1)\\\\&amp;\\space\\\\&amp;\\equiv\\\\&amp;\\space\\\\&amp;\\color{orange}{7}=\\color{yellowgreen}{4} +\\color{yellowgreen}{3} +\\color{yellowgreen}{0}\\\\&amp;\\space\\\\&amp;⟹\\\\&amp;\\space\\\\&amp;combinadic(\\color{orange}{7})=(\\color{yellowgreen}{4},\\color{yellowgreen}{3},\\color{yellowgreen}{0})\\\\\\end{eqnarray*}$$ Algorithm for UnrankNow that we’ve discussed the theory and defined the terms, lets return to the pseudocode of the unrank function: 1234567Take n and k | n = 5, k = 3, (n choose k) = 10Take the index we want | 1Move to base 0 | index - 1 = 0Calculate the dual | (n choose k) - 1 - index = 10 - 1 - 0 = 9Calculate combinadic of the dual | combinadic of dual = combinadic(9) = (4,3,2)Map to zero-based combination | (n-1) - combinadic elements = 4 - (4,3,2) = (0,1,2)Add 1 (for base 1) | (1,2,3) Things should make complete sense now. The only thing to note is how I convert my index from one-based to zero-based so I can use Dr. McCaffrey’s Unrank theory (which is zero-based), and then convert the result from zero-based back to one-based at the end for my one-based lottery generator. It was relatively simple to code this up in C#. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/// &lt;summary&gt;Returns the k-combination of (n choose k) with the provided rank&lt;/summary&gt;public int[] Unrank(int n, int k, int rank){ var dualOfZero = n - 1; // Move to base 0 rank--; // Calculate the dual var dual = n.Choose(k) - 1 - rank; // Calculate combinadic of the dual var combination = CalculateCombinadic(dual); for (int i = 0; i &lt; k; i++) { // Map to zero-based combination combination[i] = dualOfZero - combination[i]; // Add 1 (for base 1) combination[i] += 1; } return combination; /// Local Functions /// &lt;summary&gt;Calculates zero-based array of c such that maxRank = (c1 choose k-1) + (c2 choose k-2) + ... (c[of k-1] choose 1) &lt;/summary&gt; int[] CalculateCombinadic(int maxRank) { var result = new int[k]; var diminishingRank = maxRank; var reducingK = k; for (int i = 0; i &lt; k; i++) { result[i] = CalculateLargestRankBelowThreshold(n, reducingK, diminishingRank); diminishingRank -= result[i].Choose(reducingK); reducingK--; } return result; } /// &lt;summary&gt;Returns the highest rank of n2 choose k2 that is less than the threshold&lt;/summary&gt; int CalculateLargestRankBelowThreshold(int n2, int k2, int threshold) { int i = n2 - 1; while (i.Choose(k2) &gt; threshold) i--; return i; }} To run this code you’ll also need my Binomials class defining the binomial coefficient calculation and some extension functions: 123456789101112131415161718192021222324252627282930313233343536373839404142/// &lt;summary&gt;Mathematical functions for binomials&lt;/summary&gt;public static class Binomials{ // / \\ // | n | from n // | | // | k | choose k // \\ / // Kudos: https://en.wikipedia.org/wiki/Combination#Example_of_counting_combinations /// &lt;summary&gt;Calculates the Binomial Coefficient (n choose k), which is the number of unique k-sized combinations in a set of n elements.&lt;/summary&gt; /// &lt;remarks&gt;We divide before multiplying to minimise the chance of overflow.&lt;/remarks&gt; public static int CalculateBinomialCoefficient(int n, int k) { if (k &gt; n) return 0; if (k == n) return 1; double result = n; // n choose 1 for (double i = 1; i &lt; k; i++) { var numerator = n - i; double factor = numerator / (i + 1); result *= factor; } return (int)result; } //// Extensions /// &lt;summary&gt;Returns the number (n) choose k&lt;/summary&gt; public static int Choose(this int n, int k) =&gt; CalculateBinomialCoefficient(n, k); /// &lt;summary&gt;Calculates the number of unique combinations of size k from n numbers (k-combination)&lt;/summary&gt; public static int CountUniqueCombinationsOfSize(this int n, int k) =&gt; CalculateBinomialCoefficient(n, k); Algorithm for RankIt took me weeks of research, analysis, and prototyping to get to this point, so I was excited when I finally had working code to calculate the Unrank of $(n \\space choose \\space k)$. Next I needed to find an algorithm for the rank function of $(n \\space choose \\space k)$. Surely the information I needed was right alongside the theory for unrank, right? My excitement quickly faded when I discovered Dr McCaffrey’s article stopped short of discussing the rank function. Panic started to set in when I couldn’t find any code or discussion of how the Rank function works anywhere on the web. It’s as though the problem was so well understood that nobody bothered to document it. I went from having the buzz that comes with success after weeks of hard work to the dread of having to do it all again to create the second, missing algorithm. Fortunately by this point my senses for Mathematics and Science were completely re-awakened and I remembered that rank and unrank are bijections. That means I should be able to reverse the algorithm for unrank to find the algorithm for rank. One more time, here’s the pseudocode for unrank: 1234567Take n and k | n = 5, k = 3, (n choose k) = 10Take the index we want | 1Move to base 0 | index - 1 = 0Calculate the dual | (n choose k) - 1 - index = 10 - 1 - 0 = 9Calculate combinadic of the dual | combinadic of dual = combinadic(9) = (4,3,2)Map to zero-based combination | (n-1) - combinadic elements = 4 - (4,3,2) = (0,1,2)Add 1 (for base 1) | (1,2,3) Pseudocode for the Rank algorithm, one final time Reversing order we get the pseudocode for rank: 1234567Take n and k | n = 5, k = 3, (n choose k = 10)Take a combination | (1,2,3)Take 1 (for base 0) | (0,1,2)Map to combinadic | (n-1) - combination elements = 4 - (0,1,2) = (4,3,2)Calculate dual from combinadic | (4 c 3) + (3 c 2) + (2 c 1) = 4 + 3 + 2 = 9Calculate index from dual | (n c k) - 1 - dual of cmbdic = 10 - 1 - 9 = 0Add 1 (for base 1) | 1 Pseudocode for the Unrank algorithm Coding this is even easier; there’s no need to calculate the combinadic using binomial coefficients because we deconstruct it from the combination provided and the value of $n-1$. Everything plays out using addition and subtraction: 12345678910111213141516171819202122232425262728293031/// &lt;summary&gt;Returns the rank of the provided k-combination&lt;/summary&gt;public int Rank(int n, int k, int[] combination){ var dualOfZero = n - 1; var result = new int[k]; combination.CopyTo(result, 0); var dualOfCombinadic = 0; var reducingK = k; for (int i = 0; i &lt; k; i++) { // Take 1 (for base 0) result[i] -= 1; // Map to combinadic result[i] = dualOfZero - result[i]; // Calculate dual of combinadic (by accumulation) dualOfCombinadic += result[i].Choose(reducingK); reducingK--; } // Calculate the dual var dual = n.Choose(k) - 1 - dualOfCombinadic; // Add 1 (for base 1) dual++; return dual;} Return of the KingFinally I had what I needed; algorithms for rank and unrank and I could get on with building my lottery generator…except I didn’t get on with it, at least not yet. When I coded the pseudocode I had aligned my code with each step of the pseudocode to ensure I was doing it right (note how the code comments line up with the pseudocode). Now that I had an executable version of the algorithm, I could see a few places where refactoring was clearly in order; preventing the Unrank function from looping through $k$ twice for example. If I was going to refactor for more performance I should write some performance tests to measure the change in execution speed for the original and refactored functions. It was time to bust out my old friend BenchmarkDotNet. If you’ve been following my blog you may remember this old friend from my first blog post back in 2017. In the years since BenchmarkDotNet has gone from strength to strength and is now the most widely used performance measuring tool in history. Microsoft use it extensively, for example, to prevent performance regression and drive performance improvements out of the unified .NET platform. The benefits of that effort have resulted in some amazing performance gains, to the point where C# is faster than C++ in many aspects. That’s an incredible outcome. Last time I used BenchmarkDotNet I stayed within the confounds of LINQPad. This time I already had my lottery generator app already up and running somewhat in Visual Studio so it made sense to continue using it there. I refactored my Rank and Unrank algorithms into a CombinatorialNumberSystem class for consumption, which had the added benefit of simplifying how I handle values of n and k, and allowing the up-front calculation of write-once variables such as the dualOfZero. Next I created a new FastCombinatorialNumberSystem class that removed that second $k$ loop and reduced the mathematics of rebasing between zero to one, effectively merging a few pseudocode steps together. This simplified the code a lot and it now reads like real code. Pushing both implementations of Rank through some performance tests shows a significant performance boost: As you can see I ran $(5 \\space choose \\space 3)$, $(45 \\space choose \\space 6)$, and $(100 \\space choose \\space 6)$ through both implementations to give the algorithms varying degrees of workout. For Rank, the vanilla CombinatorialNumberSystem averaged less than 54 nanoseconds to compete each calculation, but the FastCombinatorialNumberSystem was 43% faster on average and took less than half the memory. What a great payoff for just a little bit of refactoring! The results for Unrank, on the other hand, were even more surprising: While the FastCombinatorialNumberSystem does perform better as expected, it’s less than 2% faster and uses the same amount of memory. I’m calling that statistically insignificant; a surprising result given this is where I removed that double loop over $k$ and expected a big boost. Perhaps the magicians of compilation-time code optimisation did the work for me and came up with essentially the same code as my refactored version. This demonstrates the value of measuring your code for actual performance gains because here I was perceiving and expecting a big boost when there wasn’t any. ConclusionHopefully this post covers everything you need as a software engineer to understand the mathematics of k-combinations and provide usable algorithms for it. I’ve stuck the code for both my regular and fast combinatorial number systems, as well as the benchmarking program that runs them, up on a Github Gist. See all the code on my Github Gist Don’t forget I’m also hosting Dr McCaffrey’s original MSDN article (due to it disappearing off the web with MSDN being shut down). The link for that is below. REHOSTED MSDN ARTICLE Generating the mth Lexicographical Element of a Mathematical Combination by Dr James McCaffrey","link":"/2021/04/10/software-algorithms-for-k-combinations/"}],"tags":[{"name":"Godot","slug":"godot","link":"/tags/godot/"},{"name":"Unity","slug":"unity","link":"/tags/unity/"},{"name":"Unreal","slug":"unreal","link":"/tags/unreal/"},{"name":"Game Maker","slug":"game-maker","link":"/tags/game-maker/"},{"name":"Sensor","slug":"sensor","link":"/tags/sensor/"},{"name":"GDScript","slug":"gdscript","link":"/tags/gdscript/"},{"name":"GUI","slug":"gui","link":"/tags/gui/"},{"name":"Menu","slug":"menu","link":"/tags/menu/"},{"name":"Visual Studio","slug":"visual-studio","link":"/tags/visual-studio/"},{"name":"MSBuild","slug":"msbuild","link":"/tags/msbuild/"},{"name":"UWP","slug":"uwp","link":"/tags/uwp/"},{"name":"GameBlog","slug":"gameblog","link":"/tags/gameblog/"},{"name":"BenchmarkDotNet","slug":"benchmarkdotnet","link":"/tags/benchmarkdotnet/"},{"name":"LINQPad","slug":"linqpad","link":"/tags/linqpad/"},{"name":"C#","slug":"c","link":"/tags/c/"},{"name":"Win2d","slug":"win2d","link":"/tags/win2d/"},{"name":"Design","slug":"design","link":"/tags/design/"},{"name":"Blog","slug":"blog","link":"/tags/blog/"},{"name":"Hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"Icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"Algorithms","slug":"algorithms","link":"/tags/algorithms/"},{"name":"Mathematics","slug":"mathematics","link":"/tags/mathematics/"},{"name":"GitHub Gist","slug":"github-gist","link":"/tags/github-gist/"}],"categories":[{"name":"gamedev","slug":"gamedev","link":"/categories/gamedev/"},{"name":"developer","slug":"developer","link":"/categories/developer/"},{"name":"gameengine","slug":"gamedev/gameengine","link":"/categories/gamedev/gameengine/"},{"name":"indiedev","slug":"gamedev/indiedev","link":"/categories/gamedev/indiedev/"},{"name":"flow","slug":"developer/flow","link":"/categories/developer/flow/"},{"name":"godot","slug":"godot","link":"/categories/godot/"},{"name":"productivity","slug":"developer/productivity","link":"/categories/developer/productivity/"},{"name":"zodproj","slug":"zodproj","link":"/categories/zodproj/"},{"name":"gdscript","slug":"godot/gdscript","link":"/categories/godot/gdscript/"},{"name":"gameengine","slug":"gamedev/indiedev/gameengine","link":"/categories/gamedev/indiedev/gameengine/"},{"name":"series","slug":"series","link":"/categories/series/"},{"name":"PauseMenu","slug":"series/pausemenu","link":"/categories/series/pausemenu/"},{"name":"tests","slug":"tests","link":"/categories/tests/"},{"name":"performance","slug":"tests/performance","link":"/categories/tests/performance/"},{"name":"design","slug":"design","link":"/categories/design/"},{"name":"process","slug":"design/process","link":"/categories/design/process/"},{"name":"hexo","slug":"hexo","link":"/categories/hexo/"},{"name":"mathematics","slug":"mathematics","link":"/categories/mathematics/"},{"name":"icarus","slug":"hexo/icarus","link":"/categories/hexo/icarus/"},{"name":"k-combinations","slug":"mathematics/k-combinations","link":"/categories/mathematics/k-combinations/"},{"name":"algorithms","slug":"algorithms","link":"/categories/algorithms/"}]}